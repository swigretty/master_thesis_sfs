\chapter{Discussion and Conclusion}\label{ch:discussion-and-conclusion}

The objective of this thesis was to demonstrate the limitations of conventional
time series methods when dealing with irregularly sampled data.
Furthermore, we aimed to determine whether Gaussian processes could serve as a
viable approach for modeling time series with irregularly spaced observations,
using the BP time series as an illustrative example.

In the theoretical section of this thesis, we elucidated that while linear
regression methods for handling correlated errors do exist, readily available
implementations are primarily designed for equispaced data.
Consequently, we introduced GP regression as a method capable of modeling
time series in continuous time.

A simulation study was subsequently conducted to investigate
the suitability of GP regression for modeling the BP time series,
which featured irregularly spaced observations. We assessed the performance of
GP regression in estimating specific target measures and compared these
results with those obtained using baseline methods.
The key findings and implications are summarized in the following section.

The final section of this thesis presents the limitations of the
simulation study and suggest potential directions
for future research and improvement.

\section{Comparison of GP Regression and Baseline Methods}

Overall, when considering all downsampling patterns and target measures,
GP regression outperforms the baseline methods. This superiority is particularly
evident when calculating the mean over small time windows, such as one-hour and one-day means.
This performance can be attributed to the fact that GP regression explicitly models
the dependencies among BP values across different time points. Consequently,
the uncertainty predictions are based on the amount of data available at time
points that are highly correlated, either positively or negatively, with the time
point of prediction.
Since the degree of correlation depends on the proximity to the prediction point,
this results in larger CI when data density is low around the prediction point.

In contrast, linear regression, while providing narrower CIs compared to GP regression,
maintains adequate CI coverage for the one-week mean under large downsampling factors.
However, linear regression does not exhibit improvement with an increase in data,
and CI coverage even decreases with more data in the case of seasonal sampling.
On one hand, the reduced CI coverage can be explained
by the fact, that bootstrapped CIs make wrong idd assumptions and are
klthus not accounting
for the AR component in the data.
On the other hand this limitation can be attributed to the inherent constraints of the linear model,
only capturing a linear trend with a perfect sinusoidal seasonal pattern and thus
exhibiting an estimation bias.
This characteristic makes the method less reliant on the amount of data available.

Spline regression, as a non-parametric method, is more flexible and data-dependent.
Thus, its performance generally improves with more data but encounters
difficulties with seasonal sampling, as it does not attempt to fit a cyclic pattern.
At high data densities with uniform sampling, spline regression produces estimates
of expected BP values that closely resemble those of GP regression,
albeit with slightly inferior CI coverage, potentially owing again to the
wrong iid assumptions of the bootstrap CIs.

Although GPs fall under the category of non-parametric methods, they offer the option to express
prior beliefs about the function of interest through the choice of kernel.
In our case, a function with a cyclic pattern with a periodicity of 24 hours,
an AR component, and a long-term trend are favored.
This choice does not impose as many constraints on the predictions as linear regression does
while simultaneously encoding more information about the function
to be fitted compared to spline regression.
These properties position GP regression as the ideal candidate for analyzing
BP time series based on irregularly spaced samples.


Seasonal sampling leads to reduced performance in all
cases but most pronounced in spline regression.
Interestingly, when confronted with seasonal sampling, more data generally
results in a reduction in CI coverage and width for linear regression.
Conversely, for GP and spline regression, CI coverage typically increases
with more data.
However, only GP regression offers adequate local CIs that expand in size
when there is less data available.

\section{Limitations and Future Work}

In the current study, GP regression is employed to estimate values generated from a GP itself.
This unique approach provides GP regression with a potential advantage over baseline methods.
To ensure a fairer comparison, we suggest the following:

\begin{itemize}
    \item Investigate entirely different methods for simulating BP values.
    Ideally, this method would also offer greater control over the simulated samples.
    Currently, when generating random samples from a GP, our ability to control the shape of the produced
    functions is limited to the choice of the kernel function.

    \item Investigate the implications of employing a misspecified kernel during estimation.
    We have consistently used the same combination of kernel types - specifically, RBF, Mat√©rn, and Periodic kernels -
    for both simulation and estimation, with only kernel hyperparameters adjusted.
    It would be intriguing to understand how sensitive predictive performance is to the mismatches in
    the kernel function.

    \item Investigate the influence
    of non-Gaussian measurement errors on predictive performance.
\end{itemize}

Additionally, expanding the scope of adversarial analysis to examine different
kernel and measurement noise combinations would provide valuable insights.
While some assumptions about the BP time series were based on real-world BP data,
the contributions of measurement errors and the autoregressive (AR) components
to real-world data remain largely uncertain.
It has been demonstrated that a larger AR component in the signal makes predictions
more challenging, and the same would apply if the simulated measurement noise were increased.
Thus, by varying the contributions of these different components,
we can gain a deeper understanding of the limits of the regression methods.

GP regression credible interval estimates were calculated based on the equal-tailed
credible interval (ETI). Another commonly used credible interval is
the highest posterior density interval (HDI), which yields different intervals,
particularly when dealing with asymmetric distributions - a scenario that might be expected for TTR.
Therefore, for the next simulation study, it is advisable to calculate both HDI and ETI
to determine which one is better suited to the specific problem at hand.

The company's specific areas of interest for further exploration include:

\begin{itemize}
    \item Simulation of a seasonal component that evolves over time.
    This can be achieved by multiplying the Periodic kernel, used so far for simulation,
    with another kernel that models this temporal evolution, such as an RBF kernel.

    \item Calculate day and night BP values. This task requires defining "day" and "night," a task that could be
    facilitated by incorporating the predicted cyclic component.

    \item Assess the computational complexity of the used regression methods

\end{itemize}


\section{Recommendations for Gaussian Process Modelling Software}

In this study, we employed Gaussian process regression using the scikit-learn
package in Python. Scikit-learn is widely known for its user-friendly interface
and versatility in general machine learning tasks; however,
it primarily provides basic Gaussian process models.

If you are seeking more advanced Gaussian process models or prefer working
in the R programming language,
the comprehensive comparison conducted by \citeauthor{erickson_comparison_2018}
offers valuable insights into various popular Gaussian process modelling software solutions.
The packages considered encompass R-based options such as DiceKriging, GPfit, laGP, and mlegp,
as well as Python-based solutions like GPy and scikit-learn.

These packages exhibit diverse approaches to determining optimal kernel hyperparameters.
For instance, GPy offers sophisticated Gaussian process models but may sacrifice speed compared to scikit-learn.
Among the R packages, GPfit distinguishes itself with its extensive parameter optimization capabilities,
rendering it suitable for smaller datasets.
Conversely, laGP is recommended for efficiently handling very large datasets.

It's noteworthy that since the publication of the referenced paper,
the author has published his how Gaussian process modelling R package named
GauPro (\citeauthor{erickson_gaupro_2023}),
adding further options to consider for your Gaussian process modelling needs.
