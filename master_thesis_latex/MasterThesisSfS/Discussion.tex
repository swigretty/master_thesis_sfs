\chapter{Discussion and Conclusion}\label{ch:discussion-and-conclusion}

\subsection{Comparison of GP Regression and Baseline Methods}

Overall, when considering different downsampling patterns and target measures,
GP regression outperforms the baseline methods. This superiority is particularly
evident when calculating the mean over small time windows, such as one-hour and one-day means.
This performance can be attributed to the fact that GP regression explicitly models
the dependencies among BP values across different time points. Consequently,
the uncertainty predictions are based on the amount of data available at time
points that are highly correlated, either positively or negatively, with the time
point of prediction.
Since the degree of correlation depends on the proximity to the prediction point,
this results in larger CI when data density is low around the prediction point.

Linear regression, on the other hand, provides the narrowest CIs and maintains adequate
CI coverage for the one-week mean under large downsampling factors. However,
linear regression does not exhibit significant improvement with an increase in data.
This can be attributed to the inherent
limitations of the linear model, which can only capture a linear trend with a perfect sinusoidal
seasonal pattern. This characteristic makes the method less reliant on the amount of data available.

In contrast, spline regression is a non-parametric method and is thus more data-dependent.
Spline regression does not yield meaningful results under large downsampling factors,
except when estimating TTR, where the locally unstable BP estimates have a relatively smaller impact.
For the same reason, spline regression does not attempt to fit a cyclic pattern and therefore struggles with seasonal sampling.

Although GPs fall under the category of non-parametric methods, they offer the option to express
prior beliefs about the function of interest through the choice of kernel.
In our case, a function with a cyclic pattern with a periodicity of 24 hours, an AR component, and a long-term trend are favored.
This choice does not impose as many constraints on the predictions as linear regression does.
These properties make GP regression the ideal candidate for analyzing BP time series based on irregularly spaced samples.


Seasonal sampling leads, in all cases, to reduced performance.
Interestingly, in the face of seasonal sampling, more data generally leads to
a reduction in Ci coverage and width for linear regression,
whereas for GP regression, CI coverage would increase,
and for spline regression, it would remain at the same level.



\section{Limitations and Future Work}

In the current study, GP regression is employed to estimate values generated
from a GP itself. This way GP regression might have an advantage over the baseline methods and
comparing their performance might to be fair.
Furthermore, the same combination of kernel types
- namely, RBF, Mat√©rn, and Periodic kernels - has been utilized for both
simulation and estimation.
Hence, when fitting a GP regression, only the kernel hyperparameters had to be found.
Although this seemed to be dificult enough, it is worthwhile to explore the
implications of employing a mispecified kernel during estimation.
Additionally,the influece of non-Gaussian measurement errors  could be investigated.
Furthermore, it might be beneficial to explore entirely different methods for
simulating BP values, ideally also offering greater control over the simulated samples.
Currently, when drawing random samples from a GP,
the only means of controlling the shape of the produced functions
lies in the choice of the kernel function.

While some assumptions about the BP time series could be made based
on real-world BP data, the contributions of the measurement
error and the AR components to the real-world data remain largely uncertain.
Expanding the scope of adversarial analysis involves exploring the impact of
varying the relative contributions of different kernels and measurement noise
on prediction performance. It has been demonstrated that a larger AR
component in the signal makes predictions harder, and the same would hold true
if the simulated measurement noise were increased.
Therefore, by varying the contributions of these different components,
we can better delineate the limits of the regression methods.

GP regression credible interval estimates were calculated
based on the equal-tailed credible interval (ETI).
Another commonly used credible interval is the highest posterior density interval (HDI),
which yields different intervals, particularly when dealing with asymmetric distributions
- a scenario that might be expected for TTR.
Therefore, for the next simulation study, it is advisable to extract both HDI and ETI to
discern which one is better suited to the specific problem at hand.

The company's particular interests for further exploration encompass the following:
\begin{itemize}
    \item Simulating a seasonal component that evolves over time.
    Achieving this could be as straightforward as multiplying the Periodic kernel,
    used thus far for simulation, with another kernel that models this temporal evolution,
    such as an RBF kernel.
    \item Calculating day and night BP values.
    This presupposes the establishment of a definition of day and night,
    which could be facilitated by incorporating the predicted cyclic component.
    \item Assessing the computational complexity of the various regression methods.
\end{itemize}





%    In the current configuration, spline regression consumes the most computational time.
%    This is because spline regession nees to be employed several time in order to identify
%    the degree of smoothing through cross validation and also estimating confidence itervals through
%    bootstrapping.

%The method used for spline regression should be fine-tuned.
%One could use ridge regression on the
%generated B-spline basis functions instead of OLS regression. In this case
%one would identify the penalty parameter $\lambda$ through cross validation and
%choose a fixed high enough number of knots.
%Instead of
%using OLS regression on the B-spline bases generated form the input times,
%choosing the number of knots of the splines through cross validation
%it is probably more common to generate B-spline bases and
%
%
%
%\begin{itemize}
%    \item Use other methods than GPs to simulate BP time series
%    \item Investigate the impact of misspecified kernel functions and the effect of adding npn-gaussian errors.
%    \item Simulate evolving seasonal component by multiply the periodic kernel with e.g. an RBF kernel
%    \item Since measurement error, the periodic and AR component are largely unknown,
%    investigate the impact of varying the relative contributions of the different kernels.
%    \item Use highest posterior density credible interval (HDI) instead of the equal-tailed interval.
%    \item Define day and night BP values with the aid of the predicted cyclic component
%    \item Investigate computational complexity of GP and baseline methods
%    \item Highest posterior density interval (HDI) (https://easystats.github.io/bayestestR/reference/hdi.html)
%    Equal-tailed interval (ETI) (https://easystats.github.io/bayestestR/reference/eti.html)
%
%\end{itemize}

