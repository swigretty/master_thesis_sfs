\chapter{Discussion and Conclusion}\label{ch:discussion-and-conclusion}

The objective of this thesis was to demonstrate the limitations of conventional
time series methods when dealing with irregularly sampled data.
Furthermore, we aimed to determine whether Gaussian processes could serve as a
viable approach for modeling time series with irregularly spaced observations,
using the BP time series as an illustrative example.

In the theoretical section of this thesis, we elucidated that while linear
regression methods for handling correlated errors do exist, readily available
implementations are primarily designed for equispaced data.
Consequently, we introduced GP regression as a method capable of modeling
time series in continuous time.

A simulation study was subsequently conducted to investigate
the suitability of GP regression for modeling the BP time series,
which featured irregularly spaced observations. We assessed the performance of
GP regression in estimating specific target measures and compared these
results with those obtained using baseline methods.
The key findings and implications are summarized in the following section.

The final section of this thesis presents the limitations of the
simulation study and suggest potential directions
for future research and improvement.


\section{Comparison of GP Regression and Baseline Methods}

Overall, when considering all downsampling patterns and target measures,
GP regression outperforms the baseline methods.
This superiority is most prominent when calculating means over shorter time intervals,
such as one-hour and one-day windows, as well as in scenarios involving seasonal sampling.
The strength of GP regression lies in its ability to provide highly
accurate local uncertainty estimates by explicitly modeling dependencies
among BP values at different time points.
Consequently, the uncertainty predictions are based on the volume of data available at time points that share
strong correlations, whether positive or negative, with the prediction time point.
The degree of correlation depends on the proximity to the prediction point,
leading to wider CIs when data density is low around the prediction point.
This feature proves particularly valuable in addressing challenges posed by seasonal
sampling and in scenarios demanding precise local uncertainty predictions.

In contrast, linear regression, while providing narrower CIs compared to GP regression,
maintains adequate CI coverage for the one-week mean under large downsampling factors.
However, linear regression does not exhibit improvement with an increase in data,
and CI coverage even decreases with more data in the case of seasonal sampling.
This reduced CI coverage can be attributed to the flawed assumption of iid observations in bootstrapped CIs.
Furthermore, this limitation stems from the constraints of the linear model,
which only captures linear trends while assuming a perfect sinusoidal seasonal pattern,
resulting in estimation bias.
These characteristics render the method less dependent on data volume.

Spline regression, as a non-parametric method, is more flexible and data-dependent.
Thus, its performance generally improves with more data but encounters
difficulties with seasonal sampling, as it does not attempt to fit a cyclic pattern.
At high data densities with uniform sampling, spline regression produces estimates
of expected BP values that closely resemble those of GP regression,
albeit with slightly inferior CI coverage, potentially also owing to the
wrong assumptions of iid observations in bootstrapped CIs.

While GPs fall within the non-parametric methodology, they offer the flexibility
to express prior beliefs about the function of interest through the choice of kernel.
In our case, functions exhibiting a cyclic pattern with 24-hour periodicity,
an AR component, and a long-term trend are favored.
This choice imposes fewer constraints on predictions than linear regression,
while encoding more information about the function to be fitted compared to spline regression.
Moreover, GP regression explicitly models the AR component, yielding more precise credible
intervals compared to the bootstrapped confidence intervals associated with the baseline methods.
Collectively, these properties position GP regression as the optimal choice for
analyzing BP time series based on irregularly spaced samples.


\section{Limitations and Future Work}

In the current study, GP regression is employed to estimate values generated from a GP itself.
This unique approach provides GP regression with a potential advantage over baseline methods.
To ensure a fairer comparison, we suggest the following:

\begin{itemize}
    \item Investigate entirely different methods for simulating BP values.
    Ideally, this method would also offer greater control over the simulated samples.
    Currently, when generating random samples from a GP, our ability to control the shape of the produced
    functions is limited to the choice of the kernel function.
    \item Investigate the implications of employing kernel function types
    during the estimation process that differ from those used for data generation.
    We have consistently used the same combination of kernel types - specifically, RBF, Mat√©rn, and Periodic kernels -
    for both simulation and estimation.
    The focus has thus been on optimizing kernel hyperparameters during GP regression.
    It would be interesting to understand how sensitive predictive performance is to the mismatches in
    the kernel function.
    \item Investigate the influence
    of non-Gaussian measurement errors on predictive performance
\end{itemize}

Additionally, expanding the scope of adversarial analysis to examine different
kernel and measurement noise combinations would provide valuable insights.
While some assumptions about the BP time series were based on real-world BP data,
the contributions of measurement errors and the autoregressive (AR) components
to real-world data remain largely uncertain.
It has been demonstrated that a larger AR component in the signal makes predictions
more challenging, and the same would apply if the simulated measurement noise were increased.
Thus, by varying the contributions of these different components,
we can gain a deeper understanding of the limits of the regression methods.

GP regression credible interval estimates were calculated based on the equal-tailed
credible interval (ETI). Another commonly used credible interval is
the highest posterior density interval (HDI), which yields different intervals,
particularly when dealing with asymmetric distributions - a scenario that might be expected for TTR.
Therefore, for the next simulation study, it is advisable to calculate both HDI and ETI
to determine which one is better suited to the specific problem at hand.

The bootstrapped CIs computed for the baseline methods in this study rely on
the assumption of iid observations.
To address the specific challenges
posed by time series data, the block bootstrap method has been developed.
The wild dependent
bootstrap has additionally been introduced by \citeauthor{shao_dependent_2010},
to handle irregularly spaced observations from nonuniform sampling
and could potentially offer
more accurate CIs for the baseline methods.
However, its applicability in the context of seasonal sampling needs
further investigation.

The company's specific areas of interest for further exploration include:

\begin{itemize}
    \item Simulation of a seasonal component that evolves over time.
    This can be achieved by multiplying the Periodic kernel, used so far for simulation,
    with another kernel that models this temporal evolution, such as an RBF kernel.

    \item Calculate day and night BP values. This task requires defining "day" and "night," a task that could be
    facilitated by incorporating the predicted cyclic component.

    \item Assess the computational complexity of the used regression methods

\end{itemize}


\subsection{Recommendations for Gaussian Process Modelling Software}

In this study, we employed Gaussian process regression using Python's scikit-learn package.
Scikit-learn is well-known for its user-friendly interface and versatility in
general machine learning tasks.
However, it primarily offers basic Gaussian process models.

If you require more advanced Gaussian process models or prefer working in the R programming language,
\citeauthor{erickson_comparison_2018} conducted a comprehensive comparison that provides valuable
insights into various popular Gaussian process modelling software solutions.
The packages considered include R-based options such as DiceKriging, GPfit, laGP, and mlegp,
as well as Python-based solutions like GPy and scikit-learn.
These packages employ diverse approaches to determine optimal kernel hyperparameters,
with the comparison primarily centered on evaluating the accuracy of predictive
mean and variance, as well as the overall versatility of each package.

For example, GPy offers advanced Gaussian process models but may trade off speed compared to scikit-learn.
Among the R packages, GPfit stands out for its extensive parameter optimization,
making it highly reliable but slower; thus, it is well-suited for smaller datasets.
In contrast, laGP is recommended for efficiently handling very large datasets,
typically containing thousands of data points.
The R package mlegp, while not as slow as GPfit, still produces very reliable results.
In our study, Gaussian processes were fitted to a maximum of 1100 data points,
where mlegp is likely a suitable choice.

It's worth noting that since the publication of the referenced paper, the author
has introduced their own Gaussian process modelling R package named GauPro
(\citeauthor{erickson_gaupro_2023}).
This addition provides further options to consider for your Gaussian process modelling needs.
