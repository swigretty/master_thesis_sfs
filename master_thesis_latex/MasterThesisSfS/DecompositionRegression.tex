%! Author = gianna
%! Date = 06.04.23


\chapter{Time Series Decomposition and Regression}

As most time series, the mean function of the BP time series is not constant in time and hence it is not stationary.
One can try to decompose the time series $Y(t)$ into a deterministic component $\mu(t)$
and a zero mean stationary process $E(t)$:
\[ Y(t)= \mu(t) + E(t) \]

This decomposition allows to extract a stationary residual $E(t)$, for which we can find a probabilistic model
using the theory of such stationary time series processes. This model in combination with $\mu(t)$ can then
for example be used to prediction the BP value $Y(t)$ at any time $t$ including confidence intervals.

The task of time series decomposition is hence to estimate $\mu(t)$, which might be an arbitrary function of $t$, from the data.
Given our knowledge of the BP time series we will start with a simple linear model that
features a linear trend and a seasonal component with known frequency $f$.
If the seasonal component is represented by a cosine $\alpha \cos(2 \pi f t - \phi)$ we obtain the
following liner model for the BP time series $Y(t)$

\begin{gather*}
Y(t) = \beta_0 + \beta_1 t + \beta_2 \cos(2 \pi f t) + \beta_3 \sin(2 \pi f t) + E(t), \\
\text{where $\beta_2 = \alpha \cos(\phi)$ and $\beta_3 = \alpha \sin(\phi)$}
\end{gather*}

If we assume observations $Y_{t_i}$ at potentially unequally spaced
time points $t_i = t_1 \dots t_n$ we can write in matrix notation:
\begin{gather*}
\mathbf{Y} = X \beta + \mathbf{E}
\end{gather*}

Where $\mathbf{Y} = [Y_{t_1}, \dots Y_{t_n}]^{\top}$ is the vector of observations,
$X = [x_{t_1}, \dots x_{t_n}]^{\top}$ is the design matrix with i-th row
$x_{t_i} = [1, t_i, \cos(2 \pi f t_i), \sin(2 \pi f t_i)]^{\top}$
and $\mathbf{E} = [E_{t_1}, \dots E_{t_n}]^{\top}$ the values of the zero-mean stationary time series.

We can use ordinary least squares to find consistent estimates $\hat{\beta}_{OLS} = (X^{\top}X) X^{\top}Y$
for the regression coefficients $\beta$ without the requirement of regularly spaced data points.
In the case of independent errors $E_{t_i}$ with constant variance $\sigma^2$ we have
$Var(\mathbf{E}) = \sigma^2 I_n$ and $\Psi = Var(\hat{\beta}_{OLS})$ can be consistently estimated with:
\begin{gather*}
\hat{\Psi} = \hat{\sigma}^2(X^{\top}X) \\
    \text{where $\hat{\sigma}^2=\frac{1}{n-p} \sum_{i = 1}^{n} (y_{t_i} - x_{t_i}^{\top} \hat{\beta}_{OLS})$ and $p=4$ in our example}
\end{gather*}

However, the assumption of uncorrelated errors $E_{t_i}$ is generally violated in the case of time series data and the
estimated covariance matrix $\hat{\Psi}$ is biased.
%and $\hat \beta_{OLS}$ is no longer an efficient estimator.

If the covariance matrix of the errors $Var(\mathbf{E}) = \Sigma$ is known,
we can use generalized least squares to obtain coefficient estimates:
\[\hat{\beta}_{GLS} = (X^{\top} \Sigma^{-1} X) X^{\top} \Sigma^{-1} Y\]
with consistent covariance matrix estimate:
\[Var(\hat{\beta}_{GLS}) = (X^{\top} \Sigma^{-1} X)^{-1}\]

If $\Sigma$ is unknown one can exploit the knowledge we have about the stationary time series process $E(t)$ to estimate it.
There are several ways to estimate $\Sigma$, $\hat \beta $ and its covariance matrix, of which i will present two.
Both methods assume an ARMA(p,q) process for $E(t)$ and equispaced time points. Hence we write $E(t)$ as $\{E_t\}$ and:

\begin{gather*}
    \Phi(B)E_t = \Theta(B)W_t, \text{where $W_t \sim WN(0, \sigma_w^2)$ and $t = 1, 2 \dots n$}
\end{gather*}


The first approach is simultaneous Maximum-Likelihood estimation of the regression coefficients and $\Sigma$.
Where based on the knowledge we have about $\{E_t\}$ the $\Sigma$ is parametrized by the ARMA coefficients.

Assuming an ARMA(2,3) process for $\{E_t\}$ we can implement this approach in R using the nlme library:

\begin{verbatim}
    library(nlme)
    cs <- corARMA(from = ~t, p=2, q=3)
    fit.gls <- gls(y ~ t + cos(2 * pi * f * t) + sin(2 * pi * f * t), corr=cs)
\end{verbatim}

The second approach to fit an OLS regression first and correct the estimated covariance matrix $\Psi$ with a
sandwich estimator.
In the presence of autocorrelation one usually estimates $\Phi = \frac{1}{n} X^{\top} \Sigma X$,
the covariance matrix of the score functions
$V_i(\beta) = x_{t_i}(y_{t_i} - x_{t_i}^{\top}\beta)$, which can then be used to derive $\Psi$:

\begin{gather*}
\Psi = Var(\hat \beta_{OLS}) = (X^{\top} X)^{-1} X^{\top} \Sigma X (X^{\top}X)^{-1} =
(\frac{1}{n} X^{\top} X)^{-1} \frac{1}{n} \Phi (\frac{1}{n} X^{\top} X)^{-1}
\end{gather*}

The general form of the estimators for $\Phi$ is:

\begin{gather*}
\hat{\Phi} = \frac{1}{n} \sum_{i,j=1}^{n} w_{|i-j|}\hat{V_i}\hat{V_j}^{\top}
\end{gather*}
where $w=[w_0, \dots w_{n-1}]^{\top}$ is a weights vector.




%
%Many authors use the word trend only for a slowly changing mean func-
%tion, such as a linear time trend, and use the term seasonal component for a mean func-
%tion that varies cyclically.




\citeauthor{brockwell_introduction_2016}
\citeauthor{zeileis_econometric_2004}



