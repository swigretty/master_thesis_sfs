%! Author = gianna
%! Date = 06.04.23


\chapter{Time Series Decomposition and Regression}

%Many authors use the word trend only for a slowly changing mean func-
%tion, such as a linear time trend, and use the term seasonal component for a mean func-
%tion that varies cyclically.

\section{Introduction}

As most time series, the mean function of the BP time series is not constant in time and hence it is not stationary.
One can try to decompose the time series $Y(t)$ into a deterministic component, the mean function $\mu(t)$
and a zero mean stationary process $E(t)$:
\[ Y(t)= \mu(t) + E(t) \]

This decomposition allows to extract a stationary residual $E(t)$, for which we can find a probabilistic model
using the theory of such stationary time series processes. The idea is to then use this model in combination
with $\mu(t)$ to predict the BP value $Y(t)$ at any time $t$ including confidence intervals.

The task of time series decomposition is hence to estimate $\mu(t)$, which might be an arbitrary function of $t$, from the data.
Given our knowledge of the BP time series we will start with a simple linear model that
features a linear trend and a seasonal component with known frequency $f$.
If the seasonal component is represented by a cosine $\alpha \cos(2 \pi f t - \phi)$ we obtain the
following liner model for the BP time series $Y(t)$

\begin{gather*}
Y(t) = \beta_0 + \beta_1 t + \beta_2 \cos(2 \pi f t) + \beta_3 \sin(2 \pi f t) + E(t), \\
\text{where $\beta_2 = \alpha \cos(\phi)$ and $\beta_3 = \alpha \sin(\phi)$}
\end{gather*}

If we assume observations $Y_{t_i}$ at potentially unequally spaced
time points $t_i = t_1 \dots t_n$ we can write in matrix notation:
\begin{gather*}
\mathbf{Y} = X \beta + \mathbf{E}
\end{gather*}

Where $\mathbf{Y} = [Y_{t_1}, \dots Y_{t_n}]^{\top}$ is the vector of observations,
$X = [x_{t_1}, \dots x_{t_n}]^{\top}$ is the design matrix with i-th row
$x_{t_i} = [1, t_i, \cos(2 \pi f t_i), \sin(2 \pi f t_i)]^{\top}$
and $\mathbf{E} = [E_{t_1}, \dots E_{t_n}]^{\top}$ the values of the zero-mean stationary time series.

We can use ordinary least squares to find unbiased and asymptotically normal estimates $\hat{\beta}_{OLS} = (X^{\top}X) X^{\top}Y$
for the regression coefficients $\beta$ without the requirement of regularly spaced data points or uncorrelated errors
$E_{t_i}$ (\citeauthor{white_asymptotic_2001}).
In the case of uncorrelated errors $E_{t_i}$ with constant variance $\sigma^2$ we have
$Var(\mathbf{E}) = \sigma^2 I_n$ and an unbiased and consistent estimator for $\Psi = Var(\hat{\beta}_{OLS})$ is given by:
\begin{gather*}
\hat{\Psi} = \hat{\sigma}^2(X^{\top}X) \\
    \text{where $\hat{\sigma}^2=\frac{1}{n-p} \sum_{i = 1}^{n} (y_{t_i} - x_{t_i}^{\top} \hat{\beta}_{OLS})$ and $p=4$ in our example}
\end{gather*}

However, the assumption of uncorrelated errors $E_{t_i}$ is generally violated in the case of time series data and the
estimated covariance matrix $\hat{\Psi}$ is biased (\citeauthor{brockwell_introduction_2016}).



\section{Generalized Least Squares}

The argument presented in this section is based on the textbook of \citeauthor{brockwell_introduction_2016}.
If the covariance matrix of the errors $Var(\mathbf{E}) = \Sigma$ is known,
we can use generalized least squares to obtain a unbiased, consistent and efficient coefficient estimate:
\[\hat{\beta}_{GLS} = (X^{\top} \Sigma^{-1} X) X^{\top} \Sigma^{-1} Y\]
with unbiased and consistent covariance matrix estimate:
\[Var(\hat{\beta}_{GLS}) = (X^{\top} \Sigma^{-1} X)^{-1}\]

If $\Sigma$ is unknown one can exploit the knowledge we have about the stationary time series process $E(t)$ to estimate it.
Two approaches to estimate $\Sigma$, $\hat \beta $ and its covariance matrix will be presented in the following subsections.
Both methods assume an ARMA(p,q) process for $E(t)$ and equispaced time points. Hence we write $E(t)$ as $\{E_t\}$ and:

\begin{gather*}
    \Phi(B)E_t = \Theta(B)W_t, \text{where $W_t \sim WN(0, \sigma_w^2)$ and $t = 1, 2 \dots n$}
\end{gather*}


\subsection{Maximum-Likelihood Estimation}

If we additionally assume $W_t \sim N(0, \sigma_w^2)$, we can simultaneously estimate the regression coefficients and $\Sigma$ by
maximizing the Gaussian likelihood:

\begin{gather*}
    L(\beta, \phi, \theta, \sigma_w^2) = (2 \pi)^{-\frac{n}{2}} (det(\Sigma_n))^{-\frac{1}{2}} exp(-\frac{1}{2}
    (\mathbf{Y}-X\beta)^{\top} \Sigma_n^{-1}(\mathbf{Y}-X\beta))
\end{gather*}

Where the covariance matrix $\Sigma_n(\theta, \phi, \sigma_w^2)$ is parametrized by the ARMA coefficients $\theta, \phi, \sigma_w^2$,
based on the order of the ARMA processed assumed for $\{E_t\}$.


Assuming an ARMA(2,3) process for $\{E_t\}$ we can implement this approach in R using the nlme library (\citeauthor{box_time_1994})
:
\begin{verbatim}
    library(nlme)
    cs <- corARMA(from = ~t, p=2, q=3)
    fit.gls <- gls(y ~ t + cos(2 * pi * f * t) + sin(2 * pi * f * t), corr=cs)
\end{verbatim}


\subsection{Sandwich Estimation}
The second approach to fit an OLS regression first and correct the estimated covariance matrix $\Psi$ with a
sandwich estimator.
In the presence of autocorrelation one usually estimates $\Phi = \frac{1}{n} X^{\top} \Sigma X$,
the covariance matrix of the score functions
$V_i(\beta) = x_{t_i}(y_{t_i} - x_{t_i}^{\top}\beta)$, which can then be used to derive $\Psi$:

\begin{equation}\label{eq:sandwich}
\Psi = Var(\hat \beta_{OLS}) = (X^{\top} X)^{-1} X^{\top} \Sigma X (X^{\top}X)^{-1} =
(\frac{1}{n} X^{\top} X)^{-1} \frac{1}{n} \Phi (\frac{1}{n} X^{\top} X)^{-1}
\end{equation}

The general form of the estimators for $\Phi$ is:

\begin{gather*}
\hat{\Phi} = \frac{1}{n} \sum_{i,j=1}^{n} w_{|i-j|}\hat{V_i}\hat{V_j}^{\top}
\end{gather*}
where $w=[w_0, \dots w_{n-1}]^{\top}$ is a weight vector.

$\Phi$ can then be plugged into the equation \ref{eq:sandwich} to obtain a
heteroskedasticity and autocorrelation consistent (HAC) covariance estimate $\hat{\Psi}_{HAC}$

\citeauthor{newey_automatic_1994}, \citeauthor{andrews_heteroskedasticity_1991} and others have suggested different approaches
for calculating the weights $w$. They all yield decreasing weights with increasing lag $l=|i-j|$.
The R sandwich package implements some of these methods to estimate $\hat{\Psi}_{HAC}$.
An introduction to the sandwich package and how it can be used
for inference is described by \citeauthor{zeileis_econometric_2004}.


\subsection{Extension to Irregularly Spaced Time Series}

Although literature and "ready to use" implementations only seem to exist for the equispaced case,
both of the approaches described above could probably be extended to the case of irregularly spaced time series.
For the Maximum-Likelihood approach one would need to adapt the parametrization of the covariance matrix $\Sigma_n$, where
the covariance at different time points would need to depend on their actual time difference rather than the lag.
A similar adaption could be done for the sandwich estimator, where the weights would need to depend on the time
difference rather than on the lag.

