%! Author = gianna
%! Date = 06.04.23


\chapter{Time Series Decomposition and Regression}\label{ch:time-series-decomposition-and-regression}

%Many authors use the word trend only for a slowly changing mean func-
%tion, such as a linear time trend, and use the term seasonal component for a mean func-
%tion that varies cyclically.


As most time series, the mean function of the BP time series is not constant in time and hence it is not stationary.
One can try to decompose the time series $Y(t)$ into a deterministic component, the mean function $\mu(t)$
and a zero mean stationary process $R(t)$. This can be expressed in the form of a regression problem:

\[ Y(t)= \mu(t) + R(t) \]

The decomposition allows to extract a stationary component $R(t)$, for which we can find a probabilistic model
using the theory of such stationary time series processes. The idea is to then use this model in combination
with an estimate of $\mu(t)$ to obtain a probability distribution of $Y^{\ast}$ at some time $t^{\ast}$.
Hence time series decomposition comes for free in regression analysis and we start with estimation of
the deterministic component $\mu(t)$ which might be an arbitrary function of $t$.

\section{Linear Regression}\label{sec:linear-regression}
Based on the knowledge we have about the system we might restrict ourselves to a family of functions for $\mu(t)$.
An obvious choice for the BP time series is the family of functions featuring a linear trend
with an additive seasonal component.
If the seasonal component is represented by a cosine of the form $\alpha \cos(2 \pi f t - \phi)$ with phase shift $\phi$
and known frequency $f$, we get the following model for the BP time series $Y(t)$:
\begin{gather*}
Y(t) = \beta_0 + \beta_1 t + \beta_2 \cos(2 \pi f t) + \beta_3 \sin(2 \pi f t) + R(t), \\
\end{gather*}
where based on the trigonometric angle sum identities we know that $\beta_2 = \alpha \cos(\phi)$ and $\beta_3 = \alpha \sin(\phi)$.

If we assume BP observations at potentially unequally spaced
time points $t_1, t_2 \dots t_n$ and $t_1 < t_2 < \dots t_n$, we can write in matrix notation:
\begin{gather*}
\mathbf{Y} = X \beta + \mathbf{R}
\end{gather*}

Where $\mathbf{Y} = [Y_{t_1}, \dots Y_{t_n}]^{\top}$ is the observed time series,
$X = [x_{t_1}, \dots x_{t_n}]^{\top} \in \mathbb{R}^{n \times 4}$ is the design matrix with i-th row, written as a column vector
$x_{t_i} = [1, t_i, \cos(2 \pi f t_i), \sin(2 \pi f t_i)]^{\top}$
and $\mathbf{R} = [R_{t_1}, \dots R_{t_n}]^{\top}$ the zero-mean stationary time series,
which we will call errors.

We can use ordinary least squares to find unbiased and asymptotically normal estimates $\hat{\beta}_{OLS} = (X^{\top}X)^{-1} X^{\top}Y$
for the regression coefficients $\beta$, without the requirement of regularly spaced data points or uncorrelated errors
$R_{t_1}, \dots, R_{t_n}$ (\citeauthor{white_asymptotic_2001}).
In the case of uncorrelated errors with constant variance $\sigma^2$ we have
$Var(\mathbf{R}) = \sigma^2 I_n$ and an unbiased and consistent estimator for $\Psi = Var(\hat{\beta}_{OLS})$ is given by:
\begin{gather*}
\hat{\Psi} = \hat{\sigma}^2(X^{\top}X)^{-1} \\
    \text{where $\hat{\sigma}^2=\frac{1}{n-p} \sum_{i = 1}^{n} (y_{t_i} - x_{t_i}^{\top} \hat{\beta}_{OLS})$ and $p=4$ in our example}
\end{gather*}

Since $\mathbf{R}$ is a time series, the assumption of uncorrelated errors is usually violated and the
covariance matrix $\hat{\Psi}$ is thus no longer unbiased (\citeauthor{brockwell_introduction_2016}).

\section{Regression with Correlated Errors}

The argument presented in this section is based on the textbook of \citeauthor{brockwell_introduction_2016}.

If the covariance matrix of the errors $Var(\mathbf{R}) = \Sigma$ is known,
we can use generalized least squares to obtain a unbiased, consistent and efficient coefficient estimate:
\[\hat{\beta}_{GLS} = (X^{\top} \Sigma^{-1} X)^{-1} X^{\top} \Sigma^{-1} Y\]
with unbiased and consistent covariance matrix estimate:
\[Var(\hat{\beta}_{GLS}) = (X^{\top} \Sigma^{-1} X)^{-1}\]

If $\Sigma$ is unknown one can exploit the knowledge we have about the stationary time series process $\mathbf{R}$ to estimate it.
The following subsections will present two approaches to estimate $\Sigma$, $\beta $ and its covariance matrix.
Both methods assume an ARMA(p,q) process for $\mathbf{R}$ and equispaced time points,
hence $\mathbf{R} = (R_t: t \in \{1, 2, \dots  n \})$ and:

\begin{gather*}
    \Phi(B)R_t = \Theta(B)W_t, \text{where $W_t \sim WN(0, \sigma_w^2)$}
\end{gather*}


\subsection{Maximum-Likelihood Estimation}\label{subsec:maximum-likelihood-estimation}

If we additionally assume $W_t \sim N(0, \sigma_w^2)$, we can simultaneously estimate the regression coefficients and $\Sigma$ by
maximizing the Gaussian likelihood:

\begin{gather*}
    L(\beta, \phi, \theta, \sigma_w^2) = (2 \pi)^{-\frac{n}{2}} |\Sigma_n|^{-\frac{1}{2}} \exp(-\frac{1}{2}
    (\mathbf{Y}-X\beta)^{\top} \Sigma_n^{-1}(\mathbf{Y}-X\beta))
\end{gather*}

Where the covariance matrix $\Sigma_n(\theta, \phi, \sigma_w^2)$ is parametrized by the coefficients $\theta, \phi, \sigma_w^2$, which
define the ARMA process assumed for $(R_t: t \in \{1, 2, \dots  n \})$.
Assuming an ARMA(2,3) process we can implement this approach in R using the nlme library (\citeauthor{box_time_1994})
:
\begin{verbatim}
    library(nlme)
    cs <- corARMA(from = ~t, p=2, q=3)
    fit.gls <- gls(y ~ t + cos(2 * pi * f * t) + sin(2 * pi * f * t), corr=cs)
\end{verbatim}


\subsection{Sandwich Estimation}
The second approach is to fit an OLS regression first and correct the estimated covariance matrix of the regression coefficients $\Psi$ with a
sandwich estimator.
In the presence of autocorrelation one usually estimates $\Phi = \frac{1}{n} X^{\top} \Sigma X$,
the covariance matrix of the scores or estimating functions
$V_i(\beta) = x_{t_i}(y_{t_i} - x_{t_i}^{\top}\beta)$, which can then be used to derive $\Psi$:

\begin{equation}\label{eq:sandwich}
\Psi = Var(\hat \beta_{OLS}) = (X^{\top} X)^{-1} X^{\top} \Sigma X (X^{\top}X)^{-1} =
(\frac{1}{n} X^{\top} X)^{-1} \frac{1}{n} \Phi (\frac{1}{n} X^{\top} X)^{-1}
\end{equation}

The general form of the estimators for $\Phi$ is:

\begin{equation}\label{eq:weights}
\hat{\Phi} = \frac{1}{n} \sum_{i,j=1}^{n} w_{|i-j|}\hat{V_i}\hat{V_j}^{\top}
\end{equation}

where $w=[w_0, \dots w_{n-1}]^{\top}$ is a weight vector and $\hat{V_i} = V_i(\hat{\beta}_{OLS})$.

Plugging $\hat{\Phi}$ into the equation \ref{eq:sandwich} one obtains the
heteroskedasticity and autocorrelation consistent (HAC) covariance estimate $\hat{\Psi}_{HAC}$.

%By formulating the problem as one of estimating $\Phi$ as a function of the scores $V_i$


\citeauthor{newey_automatic_1994}, \citeauthor{andrews_heteroskedasticity_1991} and others have suggested different approaches
for calculating the weights $w$. They all yield decreasing weights with increasing lag $l=|i-j|$.
The R sandwich package implements some of these methods to estimate $\hat{\Psi}_{HAC}$.
An introduction to the sandwich package and how it can be used
for inference is described by \citeauthor{zeileis_econometric_2004}.


\subsection{Extension to Irregularly Spaced Time Series}

Although literature and "ready to use" implementations only exist for the equispaced case,
both of the approaches described above could probably be extended to the case of irregularly spaced time series.
For the Maximum-Likelihood approach the parametrization of the covariance matrix $\Sigma_n$ as described in
\ref{subsec:maximum-likelihood-estimation} would need to be adapted,
such that the covariance of the errors at different time points depends on the actual time difference rather than the lag.
Similarly for the sandwich estimator, the weights in \ref{eq:weights} should depend on the time difference rather than on the lag.


\subsection{Confidence Intervals for the Mean Function}
The objective, as described in the introduction, is not only to estimate the mean function $\mu(t)$ of the time BP
time series but also to find confidence intervals for it.
The model for the BP time series described in \ref{sec:linear-regression} has the following mean function:
\begin{gather*}
    \mu(t) = x_{t}^{\top} \beta \\
    \text{with $x_{t} = [1, t, \cos(2 \pi f t), \sin(2 \pi f t)]^{\top}$}
\end{gather*}

Hence, we may also write $\mu(x_t)$ and its $1-\alpha$ confidence interval is:
\begin{gather*}
    x_{t}^{\top} \hat{\beta} \pm qt_{n-p}(1-\frac{\alpha}{2})  \sqrt{x_t^{\top} \Psi x_t}
\end{gather*}

where $\Psi = Var(\hat{\beta})$ is the covariance matrix of the estimated regression coefficients
and $qt_{n-p}(1-\frac{\alpha}{2})$ denotes the $1-\frac{\alpha}{2}$ quantile of the student's t-distribution of
$n-p$ degrees of freedom.

As the CI for $\mu(t)$ is based on the variance of the estimated global model parameters $\Psi$,
it cannot adapt to the local observation density.
Even if we were able to derive realistic confidence interval for the mean function of the irregularly spaced
time series, the uncertainty due to the lack of data in the proximity of a time point can still not be reflected.




TODO: Prediction interval
$1-\alpha$ prediction interval is:
\begin{gather*}
    x_{t}^{\top} \hat{\beta} \pm qt_{n-p}(1-\frac{\alpha}{2})  \sqrt{\sigma^2 + x_t^{\top} \Psi x_t}
\end{gather*}
with $\sigma^2 = \Sigma_{11}$





