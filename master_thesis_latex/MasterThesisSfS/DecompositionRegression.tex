%! Author = gianna
%! Date = 06.04.23


\chapter{Time Series Decomposition and Regression}\label{ch:time-series-decomposition-and-regression}

%Many authors use the word trend only for a slowly changing mean func-
%tion, such as a linear time trend, and use the term seasonal component for a mean func-
%tion that varies cyclically.

\section{Introduction}

As most time series, the mean function of the BP time series is not constant in time and hence it is not stationary.
One can try to decompose the time series $Y(t)$ into a deterministic component, the mean function $\mu(t)$
and a zero mean stationary process $E(t)$:
\[ Y(t)= \mu(t) + E(t) \]

This decomposition allows to extract a stationary component $E(t)$, for which we can find a probabilistic model
using the theory of such stationary time series processes. The idea is to then use this model in combination
with $\mu(t)$ to obtain a probability distribution of $Y^{\ast}$ at some time $t^{\ast}$.

The task of time series decomposition is hence to estimate $\mu(t)$, which might be an arbitrary function of $t$, from the data.
Given our knowledge of the BP time series we will start with a simple linear model that
features a linear trend and a seasonal component with known frequency $f$.
If the seasonal component is represented by a cosine $\alpha \cos(2 \pi f t - \phi)$ with phase shift $\phi$ we get the
following linear model for the BP time series $Y(t)$

\begin{gather*}
Y(t) = \beta_0 + \beta_1 t + \beta_2 \cos(2 \pi f t) + \beta_3 \sin(2 \pi f t) + E(t), \\
\text{where based on the trigonometric angle sum identity we know: $\beta_2 = \alpha \cos(\phi)$ and $\beta_3 = \alpha \sin(\phi)$}
\end{gather*}

Based on the trigonometric angle sum identities we know that $\beta_2 = \alpha \cos(\phi)$ and $\beta_3 = \alpha \sin(\phi)$

If we assume to observe the BP time series at potentially unequally spaced
time points $t_1, t_2 \dots t_n$ and $t_1 < t_2 < \dots t_n$, we can write in matrix notation:
\begin{gather*}
\mathbf{Y} = X \beta + \mathbf{E}
\end{gather*}

Where $\mathbf{Y} = [Y_{t_1}, \dots Y_{t_n}]^{\top}$ is the observed time series,
$X = [x_{t_1}, \dots x_{t_n}]^{\top} \in \mathbb{R}^{n \times 4}$ is the design matrix with i-th row
$x_{t_i} = [1, t_i, \cos(2 \pi f t_i), \sin(2 \pi f t_i)]^{\top}$
and $\mathbf{E} = [E_{t_1}, \dots E_{t_n}]^{\top}$ the zero-mean stationary time series,
which we will call errors.

We can use ordinary least squares to find unbiased and asymptotically normal estimates $\hat{\beta}_{OLS} = (X^{\top}X) X^{\top}Y$
for the regression coefficients $\beta$, without the requirement of regularly spaced data points or uncorrelated errors
$E_{t_1}, \dots, E_{t_n}$ (\citeauthor{white_asymptotic_2001}).
In the case of uncorrelated errors with constant variance $\sigma^2$ we have
$Var(\mathbf{E}) = \sigma^2 I_n$ and an unbiased and consistent estimator for $\Psi = Var(\hat{\beta}_{OLS})$ is given by:
\begin{gather*}
\hat{\Psi} = \hat{\sigma}^2(X^{\top}X) \\
    \text{where $\hat{\sigma}^2=\frac{1}{n-p} \sum_{i = 1}^{n} (y_{t_i} - x_{t_i}^{\top} \hat{\beta}_{OLS})$ and $p=4$ in our example}
\end{gather*}

Since $\mathbf{E}$ is a time series, the assumption of uncorrelated errors is usually violated and the
covariance matrix $\hat{\Psi}$ is thus no longer unbiased (\citeauthor{brockwell_introduction_2016}).

\section{Regression with Correlated Errors}

The argument presented in this section is based on the textbook of \citeauthor{brockwell_introduction_2016}.

If the covariance matrix of the errors $Var(\mathbf{E}) = \Sigma$ is known,
we can use generalized least squares to obtain a unbiased, consistent and efficient coefficient estimate:
\[\hat{\beta}_{GLS} = (X^{\top} \Sigma^{-1} X)^{-1} X^{\top} \Sigma^{-1} Y\]
with unbiased and consistent covariance matrix estimate:
\[Var(\hat{\beta}_{GLS}) = (X^{\top} \Sigma^{-1} X)^{-1}\]

If $\Sigma$ is unknown one can exploit the knowledge we have about the stationary time series process $\mathbf{E}$ to estimate it.
In the following subsections will present two approaches to estimate $\Sigma$, $\hat \beta $ and its covariance matrix.
Both methods assume an ARMA(p,q) process for $\mathbf{E}$ and equispaced time points,
hence $\mathbf{E} = (E_t: t \in \{1, 2, \dots  n \})$ and:

\begin{gather*}
    \Phi(B)E_t = \Theta(B)W_t, \text{where $W_t \sim WN(0, \sigma_w^2)$}
\end{gather*}


\subsection{Maximum-Likelihood Estimation}\label{subsec:maximum-likelihood-estimation}

If we additionally assume $W_t \sim N(0, \sigma_w^2)$, we can simultaneously estimate the regression coefficients and $\Sigma$ by
maximizing the Gaussian likelihood:

\begin{gather*}
    L(\beta, \phi, \theta, \sigma_w^2) = (2 \pi)^{-\frac{n}{2}} (det(\Sigma_n))^{-\frac{1}{2}} exp(-\frac{1}{2}
    (\mathbf{Y}-X\beta)^{\top} \Sigma_n^{-1}(\mathbf{Y}-X\beta))
\end{gather*}

Where the covariance matrix $\Sigma_n(\theta, \phi, \sigma_w^2)$ is parametrized by the coefficients $\theta, \phi, \sigma_w^2$, which
define the ARMA process assumed for $(E_t: t \in \{1, 2, \dots  n \})$.
Assuming an ARMA(2,3) process we can implement this approach in R using the nlme library (\citeauthor{box_time_1994})
:
\begin{verbatim}
    library(nlme)
    cs <- corARMA(from = ~t, p=2, q=3)
    fit.gls <- gls(y ~ t + cos(2 * pi * f * t) + sin(2 * pi * f * t), corr=cs)
\end{verbatim}


\subsection{Sandwich Estimation}
The second approach to fit an OLS regression first and correct the estimated covariance matrix of the regression coefficients $\Psi$ with a
sandwich estimator.
In the presence of autocorrelation one usually estimates $\Phi = \frac{1}{n} X^{\top} \Sigma X$,
the covariance matrix of the estimating functions
$V_i(\beta) = x_{t_i}(y_{t_i} - x_{t_i}^{\top}\beta)$, which can then be used to derive $\Psi$:

\begin{equation}\label{eq:sandwich}
\Psi = Var(\hat \beta_{OLS}) = (X^{\top} X)^{-1} X^{\top} \Sigma X (X^{\top}X)^{-1} =
(\frac{1}{n} X^{\top} X)^{-1} \frac{1}{n} \Phi (\frac{1}{n} X^{\top} X)^{-1}
\end{equation}

The general form of the estimators for $\Phi$ is:

\begin{equation}\label{eq:weights}
\hat{\Phi} = \frac{1}{n} \sum_{i,j=1}^{n} w_{|i-j|}\hat{V_i}\hat{V_j}^{\top}
\end{equation}

where $w=[w_0, \dots w_{n-1}]^{\top}$ is a weight vector and $\hat{V_i} = V_i(\hat{\beta}_{OLS})$.

Plugging $\hat{\Phi}$ into the equation \ref{eq:sandwich} one obtains the
heteroskedasticity and autocorrelation consistent (HAC) covariance estimate $\hat{\Psi}_{HAC}$.

\citeauthor{newey_automatic_1994}, \citeauthor{andrews_heteroskedasticity_1991} and others have suggested different approaches
for calculating the weights $w$. They all yield decreasing weights with increasing lag $l=|i-j|$.
The R sandwich package implements some of these methods to estimate $\hat{\Psi}_{HAC}$.
An introduction to the sandwich package and how it can be used
for inference is described by \citeauthor{zeileis_econometric_2004}.


\section{Conclusion}

\subsection{Extension to Irregularly Spaced Time Series}

Although literature and "ready to use" implementations only exist for the equispaced case,
both of the approaches described above could probably be extended to the case of irregularly spaced time series.
For the Maximum-Likelihood approach the parametrization of the covariance matrix $\Sigma_n$ as described in
\ref{subsec:maximum-likelihood-estimation} would need to be adapted,
such that the covariance of the errors at different time points depends on the absolute time difference rather than the lag.
Similarly for the sandwich estimator, the weights in \ref{eq:weights} should depend on the absolute time difference rather than on the lag.


\subsection{Confidence Intervals for the mean function}

Having identified the mean function $\mu(t) = x_{t}^{\top} \beta$ with
$x_{t} = [1, t, \cos(2 \pi f t), \sin(2 \pi f t)]^{\top}$ in our example, we also want to derive its
confidence intervals.














