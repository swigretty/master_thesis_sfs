%! Author = gianna
%! Date = 06.04.23


\chapter{Time Series Decomposition and Regression}

%Many authors use the word trend only for a slowly changing mean func-
%tion, such as a linear time trend, and use the term seasonal component for a mean func-
%tion that varies cyclically.


As most time series, the mean function of the BP time series is not constant in time and hence it is not stationary.
One can try to decompose the time series $Y(t)$ into a deterministic component, the mean function $\mu(t)$
and a zero mean stationary process $E(t)$:
\[ Y(t)= \mu(t) + E(t) \]

This decomposition allows to extract a stationary residual $E(t)$, for which we can find a probabilistic model
using the theory of such stationary time series processes. The idea is to then use this model in combination
with $\mu(t)$ to predict the BP value $Y(t)$ at any time $t$ including confidence intervals.

The task of time series decomposition is hence to estimate $\mu(t)$, which might be an arbitrary function of $t$, from the data.
Given our knowledge of the BP time series we will start with a simple linear model that
features a linear trend and a seasonal component with known frequency $f$.
If the seasonal component is represented by a cosine $\alpha \cos(2 \pi f t - \phi)$ we obtain the
following liner model for the BP time series $Y(t)$

\begin{gather*}
Y(t) = \beta_0 + \beta_1 t + \beta_2 \cos(2 \pi f t) + \beta_3 \sin(2 \pi f t) + E(t), \\
\text{where $\beta_2 = \alpha \cos(\phi)$ and $\beta_3 = \alpha \sin(\phi)$}
\end{gather*}

If we assume observations $Y_{t_i}$ at potentially unequally spaced
time points $t_i = t_1 \dots t_n$ we can write in matrix notation:
\begin{gather*}
\mathbf{Y} = X \beta + \mathbf{E}
\end{gather*}

Where $\mathbf{Y} = [Y_{t_1}, \dots Y_{t_n}]^{\top}$ is the vector of observations,
$X = [x_{t_1}, \dots x_{t_n}]^{\top}$ is the design matrix with i-th row
$x_{t_i} = [1, t_i, \cos(2 \pi f t_i), \sin(2 \pi f t_i)]^{\top}$
and $\mathbf{E} = [E_{t_1}, \dots E_{t_n}]^{\top}$ the values of the zero-mean stationary time series.

We can use ordinary least squares to find unbiased and consistent estimates $\hat{\beta}_{OLS} = (X^{\top}X) X^{\top}Y$
for the regression coefficients $\beta$ without the requirement of regularly spaced data points or independent errors $E_{t_i}$.
In the case of independent errors $E_{t_i}$ with constant variance $\sigma^2$ we have
$Var(\mathbf{E}) = \sigma^2 I_n$ and an unbiased and consistent estimator for $\Psi = Var(\hat{\beta}_{OLS})$ is given by:
\begin{gather*}
\hat{\Psi} = \hat{\sigma}^2(X^{\top}X) \\
    \text{where $\hat{\sigma}^2=\frac{1}{n-p} \sum_{i = 1}^{n} (y_{t_i} - x_{t_i}^{\top} \hat{\beta}_{OLS})$ and $p=4$ in our example}
\end{gather*}

However, the assumption of uncorrelated errors $E_{t_i}$ is generally violated in the case of time series data and the
estimated covariance matrix $\hat{\Psi}$ is biased and $\hat{\beta}_{OLS}$ is no longer efficient.

\citeauthor{brockwell_introduction_2016}


\section{Generalized Least Squares}

If the covariance matrix of the errors $Var(\mathbf{E}) = \Sigma$ is known,
we can use generalized least squares to obtain a unbiased, consistent and efficient coefficient estimate:
\[\hat{\beta}_{GLS} = (X^{\top} \Sigma^{-1} X) X^{\top} \Sigma^{-1} Y\]
with unbiased and consistent covariance matrix estimate:
\[Var(\hat{\beta}_{GLS}) = (X^{\top} \Sigma^{-1} X)^{-1}\]

If $\Sigma$ is unknown one can exploit the knowledge we have about the stationary time series process $E(t)$ to estimate it.
Two approaches to estimate $\Sigma$, $\hat \beta $ and its covariance matrix will be presented in the following subsections.
Both methods assume an ARMA(p,q) process for $E(t)$ and equispaced time points. Hence we write $E(t)$ as $\{E_t\}$ and:

\begin{gather*}
    \Phi(B)E_t = \Theta(B)W_t, \text{where $W_t \sim WN(0, \sigma_w^2)$ and $t = 1, 2 \dots n$}
\end{gather*}

\citeauthor{brockwell_introduction_2016}


\subsection{Maximum-Likelihood estimation}

If we additionally assume $W_t \sim N(0, \sigma_w^2)$, we can simultaneously etimate the regression coefficients and $\Sigma$ by
maximizing the Gaussian likelihood:

\begin{gather*}
    L(\beta, \phi, \theta, \sigma_w^2) = (2 \pi)^{-\frac{n}{2}} (det(\Sigma_n))^{-\frac{1}{2}} exp(-\frac{1}{2}
    (\mathbf{Y}-X\beta)^{\top} \Sigma_n^{-1}(\mathbf{Y}-X\beta))
\end{gather*}

Where the covariance matrix $\Sigma_n(\theta, \phi, \sigma_w^2)$ is parametrized by the ARMA coefficients $\theta, \phi, \sigma_w^2$,
based on the order of the ARMA processed assumed for $\{E_t\}$.
\citeauthor{brockwell_introduction_2016}

Assuming an ARMA(2,3) process for $\{E_t\}$ we can implement this approach in R using the nlme library:
\begin{verbatim}
    library(nlme)
    cs <- corARMA(from = ~t, p=2, q=3)
    fit.gls <- gls(y ~ t + cos(2 * pi * f * t) + sin(2 * pi * f * t), corr=cs)
\end{verbatim}
\citeauthor{box_time_1994}


\subsection{Sandwich estimation}
The second approach to fit an OLS regression first and correct the estimated covariance matrix $\Psi$ with a
sandwich estimator.
In the presence of autocorrelation one usually estimates $\Phi = \frac{1}{n} X^{\top} \Sigma X$,
the covariance matrix of the score functions
$V_i(\beta) = x_{t_i}(y_{t_i} - x_{t_i}^{\top}\beta)$, which can then be used to derive $\Psi$:

\begin{equation*}\label{eq_sandwitch}
\Psi = Var(\hat \beta_{OLS}) = (X^{\top} X)^{-1} X^{\top} \Sigma X (X^{\top}X)^{-1} =
(\frac{1}{n} X^{\top} X)^{-1} \frac{1}{n} \Phi (\frac{1}{n} X^{\top} X)^{-1}
\end{equation*}\label{eq_sandwitch}

The general form of the estimators for $\Phi$ is:

\begin{gather*}
\hat{\Phi} = \frac{1}{n} \sum_{i,j=1}^{n} w_{|i-j|}\hat{V_i}\hat{V_j}^{\top}
\end{gather*}
where $w=[w_0, \dots w_{n-1}]^{\top}$ is a weight vector.

$\Phi$ can then be plugged into the equation \ref{eq_sandwitch} to obtain a
heteroskedasticity and autocorrelation consistent (HAC) covariance estimate $\hat{\Psi}_{HAC}$

\citeauthor{newey_automatic_1994}, \citeauthor{andrews_heteroskedasticity_1991} and others have suggested different approaches
for calculating the weights $w$. They all yield decreasing weights with increasing lag $l=|i-j|$.
The R sandwich package implements some of these methods to estimate $\hat{\Psi}_{HAC}$. An introduction to the sandwich package and how it can used
for inference is desribed by \citeauthor{zeileis_econometric_2004}.







