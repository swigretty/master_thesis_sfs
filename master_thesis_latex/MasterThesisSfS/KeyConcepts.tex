%! Author = gianna
%! Date = 06.04.23

\chapter{Characteristics of Time Series}

\section{Time Series Definition}\label{sec:time-series-definition}

A potentially unevenly spaced \textbf{time series} is a sequence of observation
time and value pairs $(t_i, x_i)$ with strictly increasing observation times. Let
$\mathbb{T}$ be a set of observation time points; then the sequence of random
variables $(X_t: t \in \mathbb{T})$ or simply $(X_t)$ is a \textbf{time series
process} with observation times $t \in \mathbb{T}$. More specifically:

\begin{itemize}
    \item $(X_t: t \in \{1, 2, \dots, n\})$ refers to a discrete and equispaced
    time series of length $n$.
    \item $(X_{t_i}: i \in \{1, 2, \dots, n\})$ refers to an irregularly spaced
    time series of length $n$ with observations at time points $t_1 < t_2 < \dots
    < t_n$.
    \item $(X_{t}: t \in (0, T])$ refers to a continuous time series.
\end{itemize}

When $\mathbb{T}$ has finite length, we will often use a random column vector
$\mathbf{X}$ to refer to the time series process $(X_t)$. Sometimes a time series
model will be expressed as a random function $f: \mathbb{T} \to \mathbb{R}$ instead
of a collection of random variables. Throughout the thesis, the term time series
is used both to refer to the data $(x_t)$ and the process $(X_t)$ from which it is
generated.

\section{Moments of a Time Series}\label{sec:time_series_moments}

A time series process $(X_t)$ is usually characterized by its first and second
moments.

\begin{definition}(\citeauthor{brockwell_introduction_2016})\label{def:time_series_moments}
    The \textbf{mean function} of a time series $(X_t)$ is:
    \[
        \mu_X(t) = \ERW{X_t}
    \]
    The \textbf{covariance function} of a time series $(X_t)$ is:
    \[
        \gamma_X(r,s) = \COV{X_r, X_s} = \ERW{(X_r - \mu_X(r))(X_s-\mu_X(s))}
    \]
\end{definition}

\section{Stationarity}\label{sec:stationarity}

Given that one has only one observation $x_t$ per time point $t$, a necessary
condition to statistically learn from a time series is stationarity.

\begin{definition}(\citeauthor{brockwell_introduction_2016})
    A time series $(X_t)$ is strictly stationary if and only if the distribution of
    $(X_{t_1}, \dots, X_{t_n})$ is identical to the distribution of
    $(X_{t_{1+h}}, \dots, X_{t_{n+h}})$ for all $n \in \mathbb{N}^{+}$ and shifts
    $h \in \mathbb{Z}$:
\end{definition}

\begin{definition}(\citeauthor{brockwell_introduction_2016})
    A time series $(X_t)$ is weakly stationary if
    \[
        \mu_X(t) \text{ is independent of } t,
    \]
    and
    \[
        \gamma_X(t+h, t) \text{ is independent of } t \text{ for each } h.
    \]
\end{definition}

Whenever the term stationary is used, it is referring to weak stationarity.

\section{Special Cases of Time Series Processes}\label{sec:example-of-time-series-processes}

\begin{example} If $(X_t)$ is a \textbf{white noise} process,
    then $X_t \sim WN(0, \sigma^2)$, that is $X_t \sim F$ iid for some
    distribution $F$ with mean $0$ and varaince $\sigma^2$. A special case is Gaussian White
    noise where $W_t \sim \N(0, \sigma^2)$ and $F= \Phi$
\end{example}


\begin{example}
    An equispaced time series process $(X_t: t \in \{1,2, \dots\})$ is called an
    \textbf{autoregressive process} of order $p$ or AR($p$) if:
    \[
        X_t = \phi_1 X_{t-1} + \dots + \phi_p X_{t-p} + W_t
    \]
    where $\phi_p \neq 0$ and $(W_t)$ is a white noise process. The variable $W_t$
    is called the innovation at time $t$ and is independent of all $X_k$, $k < t$.
\end{example}

\begin{example}
    An equispaced time series process $(X_t: t \in \{1,2, \dots\})$ is called a
    \textbf{moving average process} of order $q$ or MA($q$) if:
    \[
        X_t = W_t + \theta_1 W_{t-1} + \dots \theta_q W_{t-q}
    \]
    where $\theta_q \neq 0$ and $(W_t)$ is a white noise process. The variable $W_t$
    is called the innovation at time $t$ and is independent of all $X_k$, $k < t$.
\end{example}

\begin{example}
    An equispaced time series process $(X_t: t \in \{1,2, \dots\})$ is called an
    \textbf{autoregressive moving average process} of autoregressive order $p$ and
    moving average order $q$ or ARMA($p,q$) if:
    \[
        X_t =  \phi_1 X_{t-1} + \dots + \phi_p X_{t-p} + \theta_1 W_{t-1} + \dots
        \theta_q W_{t-q} + W_t
    \]
    where $\phi_p \neq 0, \theta_q \neq 0$ and $(W_t)$ is a white noise process.
    The variable $W_t$ is called the innovation at time $t$ and is independent of all
    $X_k$, $k < t$.
\end{example}



