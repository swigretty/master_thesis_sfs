\chapter{Gaussian Process Regression}\label{ch:gaussian-process-regression}


We again consider a regression problem of mapping the input $\mathbf{x}$ to an output $f(\mathbf{x})$.
In order to solve such a problem one usually needs some additional constraints on the $f(\mathbf{x})$.
In \ref{ch:time-series-decomposition-and-regression} we restricted ourselves to the class of linear functions.
Another approach is to assign a prior probability to every possible function, where higher
probabilities are assigned to functions that are assumed to be more likely.
Inference in this Bayesian setting is then based on the posterior distribution of these
functions given some potentially noisy observations of $f(\mathbf{x})$.

This chapter first provides a definition of a Gaussian Process and then describes how it
can be used to solve a regression problem.

\section{Gaussian Process Definition}\label{sec:gaussian-process-definition}

A Gaussian process (GP) can be viewed as a gaussian distribution over functions or as an infinite set of random
variables representing the values of the function $f(\mathbf{x})$ at location $\mathbf{x}$.
The Gaussian process is thus a generalization of the Gaussian distribution and a formal definition is given
by \citeauthor{rasmussen_gaussian_2006} :

\begin{definition}[Gaussian Process]\label{def:GP}
 A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.
\end{definition}


As a (multivariate) Gaussian distribution is defined by its mean and covariance matrix, a GP is
uniquely identified by its mean $m(\mathbf{x})$ and covariance (kernel) function $k(\mathbf{x},\mathbf{x}')$.

We write

\begin{gather*}
    f(\mathbf{x}) \sim GP(m(\mathbf{x}), k(\mathbf{x},\mathbf{x}'))
\end{gather*}
with
\begin{gather*}
    m(\mathbf{x}) = \mathbb{E}[f(\mathbf{x})] \\
    k(\mathbf{x},\mathbf{x}') = \mathbb{E}[(f(\mathbf{x})-m(\mathbf{x}))(f(\mathbf{x}')-m(\mathbf{x}'))]
\end{gather*}

If we assume $X$ to be the index set or set of possible inputs of $f$, then there is a random variable
$F_x := f(\mathbf{x})$ such that for a set $A \subset X$ with $A={\mathbf{x}_1, \dots \mathbf{x}_n}$ it holds that:

\[\mathbf{F}_A = [F_{x_1}, \dots , F_{x_n}] \sim \N(\boldsymbol{\mu}_A,\,K_{AA})\]
for
\begin{gather}\label{def:Kernel-Matrix}
    K_{AA} =
    \begin{bmatrix}
        k(\mathbf{x}_1, \mathbf{x}_1) & k(\mathbf{x}_1, \mathbf{x}_2) & \dots & k(\mathbf{x}_1, \mathbf{x}_n)\\
        \vdots  &  & \vdots  & \vdots \\
        k(\mathbf{x}_n, \mathbf{x}_1)  & k(\mathbf{x}_n, \mathbf{x}_1) & \dots  & k(\mathbf{x}_n, \mathbf{x}_n)
    \end{bmatrix} \text{and }
    \boldsymbol{\mu}_A =
    \begin{bmatrix}
        \mu(\mathbf{x}_1) \\
        \vdots \\
        \mu(\mathbf{x}_n)
    \end{bmatrix}
\end{gather}
The finite marginals $F_{x_1}, \dots, F_{x_n}$ of the GP thus have a multivariate gaussian distribution.
In our running example we consider $X$ to be the time interval $T_0=[0, T]$ however it could be higher dimensional.

Note that Gaussian processes with finite index sets and hence with joint gaussian distribution is just a specific case
of GP. If we assume an ARMA process with gaussian innovations for the blood pressure time series, one can view the time series
as collection of normally distributed random variables and are thus dealing with a GP.


If we consider the linear regression case from chapter \ref{ch:time-series-decomposition-and-regression} and assume a
prior distribution
on $\boldsymbol{\beta}$, i.e. $\boldsymbol{\beta} \sim N(0, I)$ then the predictive distribution over $\boldsymbol{\mu} = X \boldsymbol{\beta}$ is Gaussian:
\[
    \boldsymbol{\mu} \sim \N(0, XX^{\top})
\]
and represents a GP with mean function $m(\mathbf{x}) = 0$ and kernel function $k(\mathbf{x}, \mathbf{x}') = \mathbf{x}^{\top}\mathbf{x}'$.
This special case of gaussian process regression with this specific kernel function is known as baysian linear regression
and will be presented in the next section.


\section{Gaussian Process Regression}
Predictions in the Bayesian regression setting is finding the posterior distribution of
$f^{\ast} := f(\mathbf{x}^{\ast})$ at some input $\mathbf{x}^{\ast}$, given some potentially noisy observations of $f(\mathbf{x})$.
This is made possible by employing a prior distribution
over the function $f(\mathbf{x})$. As shown in section \ref{sec:gaussian-process-definition}, a GP is essentially
assuming a Gaussian distribution over functions.


\subsection{Bayesian Linear Regression}\label{subsec:bayesian-linear-regression}
We consider again the linear regression model from chapter \ref{ch:time-series-decomposition-and-regression}.
However, we assume a more general setting, where the data generating process does not need to be a time series process.
We denote the mean function with $f(\mathbf{x})$ instead of $\mu(t)$ and $Y_i$ is again a noisy observations of
$f(\mathbf{x}_i)$, where the additive error $E_i$ does not necessarily need to arrive from a time series process $E(t)$.
We obtain the following data generating model:
\begin{align*}
    f(\mathbf{x}_i) &= \mathbf{x}_i^{\top}\boldsymbol{\beta}, & Y_i &= f(\mathbf{x}_i) + E_i,  & (i = 1, \dots n)
\end{align*}
with $\mathbf{x}_i \in \mathbb{R}^p$ being again the input vector and $\boldsymbol{\beta} \in \mathbb{R}^p$ is the vector with
the regression coefficients.

In matrix from:
\begin{align*}
    \mathbf{Y} = X \boldsymbol{\beta} + \mathbf{E}
\end{align*}
Where $\mathbf{Y} = [Y_{1}, \dots Y_{n}]^{\top}$ is the observed data,
$X = [\mathbf{x}_{1}, \dots \mathbf{x}_{n}]^{\top} \in \mathbb{R}^{n \times p}$ is the design matrix.
We assume again gaussian but potentially correlated errors $\mathbf{E} = [E_{1}, \dots E_{n}]^{\top}$:
\begin{gather*}
    \mathbf{E} \sim \N(0, \Sigma_n)
\end{gather*}
If $\mathbf{E}$ is an ARMA process then every element of the time series $E_{i}$
is itself a sum of innovations. Therefore, $\mathbf{E}$ is gaussian as long as it has
gaussian innovations.

The likelihood of the observations $\mathbf{Y}$ given $X$ and $\boldsymbol{\beta}$ is then:

\begin{gather*}
    p(\mathbf{Y}|X,\boldsymbol{\beta}) =
    = \frac{1}{( (2\pi)^{n/2} \sqrt {\det(\Sigma_n))}}
    \exp(-\frac{1}{2}(y - X\boldsymbol{\beta})^{\top} \Sigma_n^{-1}(y-X\boldsymbol{\beta}))
    = \N(X \boldsymbol{\beta}, \Sigma_n)
\end{gather*}

Until now the regression model is exactly the same as in chapter \ref{ch:time-series-decomposition-and-regression}.
The Bayesian approach is different in that we additionally assume a prior distribution over the
regression coefficients $\boldsymbol{\beta}$, based on what we believe are likely values for the coefficients.
To stay in the realm of gaussian processes the prior has to be guassian and we choose:

\begin{gather*}
    p(\boldsymbol{\beta}) = \N(0, \Sigma_p)
\end{gather*}
Note how the function $f(\mathbf{x}_i)=\mathbf{x}_i^{\top}\boldsymbol{\beta}$ is now no longer deterministic but a random function.

Given our observations $\mathbf{Y}$  we can use Bayes' theorem to calculate the posterior distribution over $\boldsymbol{\beta}$:
\begin{gather*}
    p(\boldsymbol{\beta}| \mathbf{Y}, X) = \frac{p(\mathbf{Y},\boldsymbol{\beta}|X)}{p(\mathbf{Y}|X)} =
    \frac{p(\mathbf{Y}|X,\boldsymbol{\beta})p(\boldsymbol{\beta})}{p(\mathbf{Y}|X)}
\end{gather*}

One approach is to just plug in the expressions for
$p(\mathbf{Y}|X,\boldsymbol{\beta})$ and $p(\boldsymbol{\beta}|\mathbf{Y}, X)$ from above, with:

\begin{gather*}
    p(\mathbf{Y}|X) = \int p(\mathbf{Y}|X,\boldsymbol{\beta}) p(\boldsymbol{\beta}) d\boldsymbol{\beta} = \N(0, X \Sigma_p X^{\top} + \Sigma_n)
\end{gather*}

Or it can be helpful to combine the coefficients and the observations into a single random vector with
multivariate normal distribution:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        \boldsymbol{\beta}
    \end{bmatrix}
    = \begin{bmatrix} X \\ I_p \end{bmatrix} \boldsymbol{\beta} + \begin{bmatrix} I_n \\ 0 \end{bmatrix}  \mathbf{E}
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        \vdots \\
        \vdots \\
        0 \\
        0 \\
        \vdots \\
        0
        \end{bmatrix},
        \left[
        \begin{array}{ c:c c c }
            \begin{matrix}
                & & \\
                & & \\
                & & \\
                & X \Sigma_p X^{\top} + \Sigma_n & \\
                & & \\
                & & \\
                & & \\
            \end{matrix}
            & \begin{matrix} \\ \\ \\ X \Sigma_p  \\ \\ \\ \end{matrix} \\
            \hdashline \\
            \begin{matrix} &  \Sigma_p X^{\top} & \end{matrix} & \Sigma_p
        \end{array}
        \right]
        \right)
    = p(\mathbf{Y}, \boldsymbol{\beta} | X)
\end{gather}

with $\Sigma_p X^{\top} + \Sigma_n \in \mathbb{R}^{n\times n}$ and $\Sigma_p X^{\top} \in \mathbb{R}^{p\times n}$.

To find now the posterior distribution $p(\boldsymbol{\beta} | \mathbf{Y}, X)$ one can use the rules for deriving conditional
distributions for multivariate Gaussian's.

\begin{theorem}\label{thrm:Gaussian-Conditioning} (\citeauthor{von_mises_mathematical_1964})

Let $\mathbf{A} \sim \N(\boldsymbol{\mu}_A, \Sigma_{AA})$ and $\mathbf{B} \sim \N(\boldsymbol{\mu}_B, \Sigma_{BB})$ be
Gaussian random vectors with the following joint distribution:

\begin{gather}
    p(\mathbf{A}, \mathbf{B}) = \N(
    \begin{bmatrix}
        \boldsymbol{\mu}_A \\
        \boldsymbol{\mu}_B
    \end{bmatrix},
    \begin{bmatrix}
        \Sigma_{AA} & \Sigma_{AB} \\
        \Sigma_{BA} & \Sigma_{BB}
    \end{bmatrix}
\end{gather}

Then the conditional distribution $p(\mathbf{B} | \mathbf{A}=a)$ is also normally distributed
with mean $\bar{\boldsymbol{\mu}}$ and covariance $\bar{\Sigma}$ of the following form:

\begin{align}
    \bar{\Sigma} = \Sigma_{B B} - \Sigma_{B A} \Sigma_{A A}^{-1} \Sigma_{A B} & & \bar{\boldsymbol{\mu}} = \boldsymbol{\mu}_{B} + \Sigma_{BA} \Sigma_{AA}^{-1}(a - \boldsymbol{\mu}_A)
\end{align}
\end{theorem}

Using theorem \ref{thrm:Gaussian-Conditioning} the posterior distribution over $\boldsymbol{\beta}$ is then given by:
\begin{gather*}
    p(\boldsymbol{\beta} | \mathbf{Y}=y, X) \sim \N(\bar{\boldsymbol{\mu}}, \bar{\Sigma}), \\
    \bar{\Sigma} = \Sigma_{p} - \Sigma_p X^{\top}(X \Sigma_p X^{\top} + \Sigma_n)^{-1} X  \Sigma_p, \\
    \bar{\boldsymbol{\mu}} = \boldsymbol{\mu}_{\beta} + \Sigma_p X^{\top}(X \Sigma_p X^{\top} + \Sigma_n)^{-1}y
\end{gather*}

The expression for the posterior mean and covariance matrix can be further simplified using Woodbury matrix identity
and we obtain:
\begin{align}\label{def:conditional-mean-var}
    \bar{\Sigma} = (X^{\top}\Sigma_n^{-1}X + \Sigma_p^{-1})^{-1} & & \bar{\boldsymbol{\mu}} = \bar{\Sigma} X^{\top} \Sigma_n^{-1} y
\end{align}

Since $f(\mathbf{x}) = \mathbf{x}^{\top}\boldsymbol{\beta}$, one can use the posterior mean and covariance matrix from
\ref{def:conditional-mean-var} to obtain the predictive distribution of $f^{\ast} := f(\mathbf{x}^{\ast})$ at $\mathbf{x}^{\ast}$
given our observations:
\begin{align}\label{def:predictive-dist}
    p(f^{\ast} | \mathbf{Y}, X, \mathbf{x}^{\ast}) = \N(\mathbf{x}^{\ast^{\top}} \bar{\boldsymbol{\mu}}, \mathbf{x}^{\ast^{\top}} \bar{\Sigma} \mathbf{x}^{\ast}) \\
\end{align}

One can also use the rules for conditioning to directly derive $f^{\ast} | \mathbf{Y}, X, \mathbf{x}^{\ast}$.
Similar to before we can write the joint distribution $p(\mathbf{Y}, f^{\ast}| X, \mathbf{x}^{\ast})$:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        f^{\ast}
    \end{bmatrix}
    = \begin{bmatrix} X \\ x^{\ast} \end{bmatrix} \boldsymbol{\beta} + \begin{bmatrix} I_n \\ 0 \end{bmatrix}  \mathbf{E}
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        \vdots \\
        \vdots \\
        0 \\
        0
        \end{bmatrix},
        \left[
        \begin{array}{ c:c c c }
            \begin{matrix}
                & & \\
                & & \\
                & & \\
                & X \Sigma_p X^{\top} + \Sigma_n & \\
                & & \\
                & & \\
                & & \\
            \end{matrix}
            & \begin{matrix} \\ \\ \\ X \Sigma_p \mathbf{x}^{\ast} \\ \\ \\ \end{matrix} \\
            \hdashline \\
            \begin{matrix} &  \mathbf{x}^{\ast^{\top}}  \Sigma_p X^{\top} & \end{matrix} & \Sigma_p
        \end{array}
        \right]
        \right)
    = p(\mathbf{Y}, f^{\ast}| X, \mathbf{x}^{\ast})
\end{gather}

The expression in \ref{def:predictive-dist} can then be derived using theorem \ref{thrm:Gaussian-Conditioning} on
conditioning of multivariate Gaussian's.

\subsection{Gaussian Process Regression}
The linear model discussed so far, with a cyclic component represented by a cosine and a linear trend component,
might be an evident first guess. However, it is very unlikely that the BP values are exactly following this pattern.
Instead of reducing the function space to this specific class of linear functions, we may use our domain knowledge
to tell which functions of the infinite space of all functions are more likely to have generated our data.
As these functions are not characterized with explicit sets of
parameters, this approach belongs to the branch of non-parametric modelling.
By abandoning the parameters $\boldsymbol{\beta}$, Gaussian process regression
directly aims for the predictive distribution of $f^{\ast} := f(\mathbf{x}^{\ast})$ at $\mathbf{x}^{\ast}$ given our observations.

Starting with the Bayesian linear regression example from last section and transforming it into a GP regression
problem, we recall that the distribution of $\mathbf{F}_X = [f(\mathbf{x}_1) \dots f(\mathbf{x}_n))]^{\top}$ with given $X = [\mathbf{x}_1 \dots \mathbf{x}_n]^{\top}$ is:
\begin{gather*}
    \mathbf{F}_X \sim \N(0,  X \Sigma_p X^{\top})
\end{gather*}

Alternatively this can be written as a distribution over the function $f(\mathbf{x})$:

\begin{gather*}
    f(\mathbf{x}) \sim GP(0, k(\mathbf{x}, \mathbf{x}'))
\end{gather*}
where $k(\mathbf{x},\mathbf{x}')$ needs to be chosen such that for an input X we obtain $K_{XX} =  X \Sigma_p X^{\top}$.

For example if we assume $\Sigma_p = \sigma_p I$, we would choose $k(\mathbf{x},\mathbf{x}') = \sigma_p \mathbf{x}^{\top} \mathbf{x}'$, with the
input pairs $\mathbf{x}$ and $\mathbf{x}'$ only entering as a dot product.
Note that since $\Sigma_p$ is postive definite we can define $\Sigma_p^{\frac{1}{2}} = (\Sigma_p^{\frac{1}{2}})^2=\Sigma_p$.
Then defining $\phi(\mathbf{x}) = \Sigma_p^{\frac{1}{2}} \mathbf{x}$ the kernel function becomes $k(\mathbf{x}, \mathbf{x}') = \phi(\mathbf{x})^{\top} \phi(\mathbf{x}')$,
which is again the dot product of pairs of $\phi(\mathbf{x})$.
This shows that Baysian linear regression with transformed inputs $\phi(\mathbf{x})$ and prior covariance matrix $\Sigma_p = I$,
has the same effect as choosing a more complicated $\Sigma_p$ and leaving the inputs untouched.


%For example if we assume $\Sigma_p = \sigma_p I$, we would choose $k(x,x') = \sigma_p x^{\top} x'$.

%Assuming $\Sigma_p = \sigma_p I$ and $\Sigma_n = \sigma_n I)$ we get for the kernel function:
%
%\begin{gather*}
%    k(x, x') = \sigma_p x^{\top} x' +  \mathbbm{1}_{x = x'}\sigma_n
%\end{gather*}

%δ pq is a Kronecker delta which is one iff p = q and zero otherwise.

Combining $f^{\ast}$ and $\mathbf{Y}$ into a single random vector we can use the theorem \ref{thrm:Gaussian-Conditioning}
to arrive at the same posterior predictive distribution
$p(f^{\ast} | \mathbf{Y}, X, \mathbf{x}^{\ast})$ as presented in \ref{def:predictive-dist}.
The joint distribution of $f^{\ast}$ and $\mathbf{Y}$ can be expressed as follows:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        f^{\ast}
    \end{bmatrix} =
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        0
        \end{bmatrix},
        \begin{bmatrix}
        K_{XX} + \Sigma_n & K_{X\mathbf{x}^{\ast}} \\
        K_{\mathbf{x}^{\ast}X} & K_{\mathbf{x}^{\ast}\mathbf{x}^{\ast}}
        \end{bmatrix}
        \right)
    = p(\mathbf{Y}, f^{\ast}| X, \mathbf{x}^{\ast})
\end{gather}

where:
\begin{gather*}
    K_{XX} =
    \begin{bmatrix}
        k(\mathbf{x}_1, \mathbf{x}_1) & k(\mathbf{x}_1, \mathbf{x}_2) & \dots & k(\mathbf{x}_1, \mathbf{x}_n)\\
        \vdots  &  & \vdots  & \vdots \\
        k(\mathbf{x}_n, \mathbf{x}_1)  & k(\mathbf{x}_n, \mathbf{x}_1) & \dots  & k(\mathbf{x}_n, \mathbf{x}_n)
    \end{bmatrix}, \\
    K_{X\mathbf{x}^{\ast}} = K_{\mathbf{x}^{\ast}X}^{\top} =
    \begin{bmatrix}
        k(\mathbf{x}_1, \mathbf{x}^{\ast}) \\
        \vdots \\
        k(\mathbf{x}_n,  \mathbf{x}^{\ast})
    \end{bmatrix} \text{ and }
    K_{\mathbf{x}^{\ast}\mathbf{x}^{\ast}} = k(\mathbf{x}^{\ast}\mathbf{x}^{\ast})
\end{gather*}

Unlike in chapter \ref{ch:time-series-decomposition-and-regression}, $f(\mathbf{x})$ is no longer assumed to be a
deterministic function. This way, GP regression allows us to treat $\mathbf{E}$ not simply as an error term but an actual part of
our signal which we can predict. If $\mathbf{E}$ is not independent noise but for example a time series, where
the elements of $\mathbf{E}$ are correlated, we want to leverage the information we have about an unobserved
time point given our observations.
Hence, we are not interested in the posterior distribution of $f^{\ast}$ only, but also of
$Y^{\ast} := Y(\mathbf{x}^{\ast}) = f(\mathbf{x}^{\ast}) + E(\mathbf{x}^{\ast})$.

Recall the prior distribution over $\mathbf{Y}$:
\begin{gather*}
    \mathbf{Y}|X \sim \N(0,  X \Sigma_p X^{\top} + \Sigma_n) \\
\end{gather*}

Alternatively, this can be expressed as a distribution over the function $Y(\mathbf{x})$:
\begin{gather*}
    Y(\mathbf{x}) \sim GP(0, k(\mathbf{x}, \mathbf{x}'))
\end{gather*}
The kernel function $k(\mathbf{x},\mathbf{x}')$ needs to be chosen such that for an index set X we obtain $K_{XX} =  X \Sigma_p X^{\top} + \Sigma_n$.
One can then follow again the same procedure as before and combine $Y^{\ast}$ and $\mathbf{Y}$ into a single random vector:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        Y^{\ast}
    \end{bmatrix}
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        0
        \end{bmatrix},
        \begin{bmatrix}
        K_{XX} & K_{X\mathbf{x}^{\ast}} \\
        K_{\mathbf{x}^{\ast}X} & K_{\mathbf{x}^{\ast}\mathbf{x}^{\ast}}
        \end{bmatrix}
        \right)
    = p(\mathbf{Y}, f^{\ast}| X, \mathbf{x}^{\ast})
\end{gather}

The predictive distribution $p(Y^{\ast} | \mathbf{Y}, X, \mathbf{x}^{\ast})$ is then again derived by conditioning.

Note how until now we have still assumed $\Sigma_n$, the covariance matrix of $\mathbf{E}$, to be known.
However, deriving $\Sigma_n$ for an ARMA process with irregularly spaced samples is not straight forward, as has already
been shown in chapter \ref{ch:time-series-decomposition-and-regression}. The next section will illustrate how choosing a
specific kernel function solves this problem.

\subsection{Kernel Fuctions}\label{subsec:kernel}

Choosing a prior distribution over $f(\mathbf{x})$ or $Y(\mathbf{x})$ boils down to choosing a kernel function.
The kernel or covariance function encodes assumptions about correlation between arbitrary input pairs $\mathbf{x}$ and $\mathbf{x}'$.
This enforces certain properties on the kernel function, which are symmetry and positive definiteness.
%which enforces
%certain properties on these kernel functions. A valid kernel function must be:
%
%\begin{itemize}
%    \item symmetric
%    \item postive definite
%\end{itemize}
%

Additionally one can combine valid covariance functions, $k1$ and $k2$ into new covariance functions based on the following rules:
\begin{itemize}
    \item
\end{itemize}







TODO
One could hence also assume additional measurement noise:
\begin{align*}
    Z(\mathbf{x}) = Y(\mathbf{x}) + \epsilon = f(\mathbf{x}) + E(\mathbf{x}) + \epsilon  && \epsilon \sim \N(0, \sigma_n^{2})
\end{align*}




TODO
This still  assumes that $\Sigma_n$ is known.



