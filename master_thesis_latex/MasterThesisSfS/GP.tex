\chapter{Gaussian Process Regression}\label{ch:gaussian-process-regression}


We again consider a regression problem of mapping the input $x$ to an output $f(x)$.
In order to solve such a problem one usually needs some additional constraints on the $f(x)$.
In \ref{ch:time-series-decomposition-and-regression} we restricted ourselves to the class of linear functions.
Another approach is to assign a prior probability to every possible function, where higher
probabilities are assigned to functions that are assumed to be more likely.
Inference in this Bayesian setting is then based on the posterior distribution of these
functions given some potentially noisy observations of $f(x)$.

This chapter first provides a definition of a Gaussian Process and then describes how they
can be used to solve a regression problem.

\section{Gaussian Process Definition}\label{sec:gaussian-process-definition}

A Gaussian process (GP) can be viewed as a gaussian distribution over functions or as an infinite set of random
variables representing the values of the function $f(x)$ at location $x$.
The Gaussian process is thus a generalization of the Gaussian distribution and a formal definition is given
by \citeauthor{rasmussen_gaussian_2006} :

\begin{definition}[Gaussian Process]\label{def:GP}
 A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.
\end{definition}


As a (multivariate) Gaussian distribution is defined by its mean and covariance matrix, a GP is
uniquely identified by its mean $m(x)$ and covariance (kernel) function $k(x,x')$.

We write

\begin{gather*}
    f(x) \sim GP(m(x), k(x,x'))
\end{gather*}
with
\begin{gather*}
    m(x) = \mathbb{E}[f(x)] \\
    k(x,x') = \mathbb{E}[(f(x)-m(x))(f(x')-m(x'))]
\end{gather*}

If we assume $X$ to be the index set or set of possible inputs of $f$, then there is a random variable
$F_x := f(x)$ such that for a set $A \subset X$ with $A={x_1, \dots x_n}$ it holds that:

\[\mathbf{F}_A = [F_{x_1}, \dots , F_{x_n}] \sim \N(\boldsymbol{\mu}_A,\,K_{AA})\]
for
\begin{gather}\label{def:Kernel-Matrix}
    K_{AA} =
    \begin{bmatrix}
        k(x_1, x_1) & k(x_1, x_2) & \dots & k(x_1, x_n)\\
        \vdots  &  & \vdots  & \vdots \\
        k(x_n, x_1)  & k(x_n, x_1) & \dots  & k(x_n, x_n)
    \end{bmatrix} \text{and }
    \boldsymbol{\mu}_A =
    \begin{bmatrix}
        \mu(x_1) \\
        \vdots \\
        \mu(x_n)
    \end{bmatrix}
\end{gather}
The finite marginals $F_{x_1}, \dots, F_{x_n}$ of the GP thus have a multivariate gaussian distribution.
In our running example we consider $X$ to be the time interval $T_0=[0, T]$ however it could be higher dimensional.

Note that Gaussian processes with finite index sets and hence with joint gaussian distribution is just a specific case
of GP. If we assume an ARMA process with gaussian innovations for the blood pressure time series, one can view the time series
as collection of normally distributed random variables and are thus dealing with a GP.


If we consider the linear regression case from chapter \ref{ch:time-series-decomposition-and-regression} and assume a
prior distribution
on $\boldsymbol{\beta}$, i.e. $\boldsymbol{\beta} \sim N(0, I)$ then the predictive distribution over $\boldsymbol{\mu} = X \boldsymbol{\beta}$ is Gaussian:
\[
    \boldsymbol{\mu} \sim \N(0, XX^{\top})
\]
and represents a GP with mean function $m(x) = 0$ and kernel function $k(x, x') = x^{\top}x'$.
This special case of gaussian process regression with this specific kernel function is known as baysian linear regression
and will be presented in the next section.


\section{Gaussian Process Regression}
Predictions in the Bayesian regression setting is finding the posterior distribution of
$f^{\ast} := f(x^{\ast})$ at some input $x^{\ast}$, given some potentially noisy observations of $f(x)$.
This is made possible by employing a prior distribution
over the function $f(x)$. As shown in section \ref{sec:gaussian-process-definition}, a GP is essentially
assuming a Gaussian distribution over functions.


\subsection{Bayesian Linear Regression}\label{subsec:bayesian-linear-regression}

We consider again the linear regression model from chapter \ref{ch:time-series-decomposition-and-regression}.
However, we assume a more general setting, where the data generating process does not need to be a time series process.
We denote the mean function with $f(x)$ instead of $\mu(t)$ and $Y_i$ is again a noisy observations of
$f(x_i)$, where the additive error $E_i$ does not necessarily need to arrive from a time series process $E(t)$.
We obtain the following data generating model:
\begin{align*}
    f(x_i) &= x_i^{\top}\boldsymbol{\beta}, & Y_i &= f(x_i) + E_i,  & (i = 1, \dots n)
\end{align*}
with $x_i \in \mathbb{R}^p$ being again the input vector and $\boldsymbol{\beta} \in \mathbb{R}^p$ is the vector with
the regression coefficients.

In matrix from:
\begin{align*}
    \mathbf{Y} = X \boldsymbol{\beta} + \mathbf{E}
\end{align*}
Where $\mathbf{Y} = [Y_{1}, \dots Y_{n}]^{\top}$ is the observed data,
$X = [x_{1}, \dots x_{n}]^{\top} \in \mathbb{R}^{n \times p}$ is the design matrix.
We assume again gaussian but potentially correlated errors $\mathbf{E} = [E_{1}, \dots E_{n}]^{\top}$:
\begin{gather*}
    \mathbf{E} \sim \N(0, \Sigma_n)
\end{gather*}
If $\mathbf{E}$ is an ARMA process then every element of the time series $E_{i}$
is itself a sum of innovations. Therefore, $\mathbf{E}$ is gaussian as long as it has
gaussian innovations.

The likelihood of the observations $\mathbf{Y}$ given $X$ and $\boldsymbol{\beta}$ is then:

\begin{gather*}
    p(\mathbf{Y}|X,\boldsymbol{\beta}) =
    = \frac{1}{( (2\pi)^{n/2} \sqrt {\det(\Sigma_n))}}
    \exp(-\frac{1}{2}(y - X\boldsymbol{\beta})^{\top} \Sigma_n^{-1}(y-X\boldsymbol{\beta}))
    = \N(X \boldsymbol{\beta}, \Sigma_n)
\end{gather*}

Until now the regression model is exactly the same as in chapter \ref{ch:time-series-decomposition-and-regression}.
The Bayesian approach is different in that we additionally assume a prior distribution over the
regression coefficients $\boldsymbol{\beta}$, based on what we believe are likely values for the coefficients.
To stay in the realm of gaussian processes the prior has to be guassian and we choose:

\begin{gather*}
    p(\boldsymbol{\beta}) = \N(0, \Sigma_p)
\end{gather*}
Note how the function $f(x_i)=x_i^{\top}\boldsymbol{\beta}$ is now no longer deterministic but a random function.

Given our observations $\mathbf{Y}$  we can use Bayes' theorem to calculate the posterior distribution over $\boldsymbol{\beta}$:
\begin{gather*}
    p(\boldsymbol{\beta}| \mathbf{Y}, X) = \frac{p(\mathbf{Y},\boldsymbol{\beta}|X)}{p(\mathbf{Y}|X)} =
    \frac{p(\mathbf{Y}|X,\boldsymbol{\beta})p(\boldsymbol{\beta})}{p(\mathbf{Y}|X)}
\end{gather*}

One approach is to just plug in the expressions for
$p(\mathbf{Y}|X,\boldsymbol{\beta})$ and $p(\boldsymbol{\beta}|\mathbf{Y}, X)$ from above, with:

\begin{gather*}
    p(\mathbf{Y}|X) = \int p(\mathbf{Y}|X,\boldsymbol{\beta}) p(\boldsymbol{\beta}) d\boldsymbol{\beta} = \N(0, X \Sigma_p X^{\top} + \Sigma_n)
\end{gather*}

Or it can be helpful to combine the coefficients and the observations into a single random vector with
multivariate normal distribution:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        \boldsymbol{\beta}
    \end{bmatrix}
    = \begin{bmatrix} X \\ I_p \end{bmatrix} \boldsymbol{\beta} + \begin{bmatrix} I_n \\ 0 \end{bmatrix}  \mathbf{E}
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        \vdots \\
        \vdots \\
        0 \\
        0 \\
        \vdots \\
        0
        \end{bmatrix},
        \left[
        \begin{array}{ c:c c c }
            \begin{matrix}
                & & \\
                & & \\
                & & \\
                & X \Sigma_p X^{\top} + \Sigma_n & \\
                & & \\
                & & \\
                & & \\
            \end{matrix}
            & \begin{matrix} \\ \\ \\ X \Sigma_p  \\ \\ \\ \end{matrix} \\
            \hdashline \\
            \begin{matrix} &  \Sigma_p X^{\top} & \end{matrix} & \Sigma_p
        \end{array}
        \right]
        \right)
    = p(\mathbf{Y}, \boldsymbol{\beta} | X)
\end{gather}

with $\Sigma_p X^{\top} + \Sigma_n \in \mathbb{R}^{n\times n}$ and $\Sigma_p X^{\top} \in \mathbb{R}^{p\times n}$.

To find now the posterior distribution $p(\boldsymbol{\beta} | \mathbf{Y}, X)$ one can use the rules for deriving conditional
distributions for multivariate Gaussian's.

\begin{theorem}\label{thrm:Gaussian-Conditioning} (\citeauthor{von_mises_mathematical_1964})

Let $\mathbf{A} \sim \N(\boldsymbol{\mu}_A, \Sigma_{AA})$ and $\mathbf{B} \sim \N(\boldsymbol{\mu}_B, \Sigma_{BB})$ be
Gaussian random vectors with the following joint distribution:

\begin{gather}
    p(\mathbf{A}, \mathbf{B}) = \N(
    \begin{bmatrix}
        \boldsymbol{\mu}_A \\
        \boldsymbol{\mu}_B
    \end{bmatrix},
    \begin{bmatrix}
        \Sigma_{AA} & \Sigma_{AB} \\
        \Sigma_{BA} & \Sigma_{BB}
    \end{bmatrix}
\end{gather}

Then the conditional distribution $p(\mathbf{B} | \mathbf{A}=a)$ is also normally distributed
with mean $\bar{\boldsymbol{\mu}}$ and covariance $\bar{\Sigma}$ of the following form:

\begin{align}
    \bar{\Sigma} = \Sigma_{B B} - \Sigma_{B A} \Sigma_{A A}^{-1} \Sigma_{A B} & & \bar{\boldsymbol{\mu}} = \boldsymbol{\mu}_{B} + \Sigma_{BA} \Sigma_{AA}^{-1}(a - \boldsymbol{\mu}_A)
\end{align}
\end{theorem}

Using theorem \ref{thrm:Gaussian-Conditioning} the posterior distribution over $\boldsymbol{\beta}$ is then given by:
\begin{gather*}
    p(\boldsymbol{\beta} | \mathbf{Y}=y, X) \sim \N(\bar{\boldsymbol{\mu}}, \bar{\Sigma}), \\
    \bar{\Sigma} = \Sigma_{p} - \Sigma_p X^{\top}(X \Sigma_p X^{\top} + \Sigma_n)^{-1} X  \Sigma_p, \\
    \bar{\boldsymbol{\mu}} = \boldsymbol{\mu}_{\beta} + \Sigma_p X^{\top}(X \Sigma_p X^{\top} + \Sigma_n)^{-1}y
\end{gather*}

The expression for the posterior mean and covariance matrix can be further simplified using Woodbury matrix identity
and we obtain:
\begin{align}\label{def:conditional-mean-var}
    \bar{\Sigma} = (X^{\top}\Sigma_n^{-1}X + \Sigma_p^{-1})^{-1} & & \bar{\boldsymbol{\mu}} = \bar{\Sigma} X^{\top} \Sigma_n^{-1} y
\end{align}

Since $f(x) = x^{\top}\boldsymbol{\beta}$, one can use the posterior mean and covariance matrix from
\ref{def:conditional-mean-var} to obtain the predictive distribution of $f^{\ast} := f(x^{\ast})$ at $x^{\ast}$
given our observations:
\begin{align}\label{def:predictive-dist}
    p(f^{\ast} | \mathbf{Y}, X, x^{\ast}) = \N(x^{\ast^{\top}} \bar{\boldsymbol{\mu}}, x^{\ast^{\top}} \bar{\Sigma} x^{\ast}) \\
\end{align}

One can also use the rules for conditioning to directly derive $f^{\ast} | \mathbf{Y}, X, x^{\ast}$.
Similar to before we can write the joint distribution $p(\mathbf{Y}, f^{\ast}| X, x^{\ast})$:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        f^{\ast}
    \end{bmatrix}
    = \begin{bmatrix} X \\ x^{\ast} \end{bmatrix} \boldsymbol{\beta} + \begin{bmatrix} I_n \\ 0 \end{bmatrix}  \mathbf{E}
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        \vdots \\
        \vdots \\
        0 \\
        0
        \end{bmatrix},
        \left[
        \begin{array}{ c:c c c }
            \begin{matrix}
                & & \\
                & & \\
                & & \\
                & X \Sigma_p X^{\top} + \Sigma_n & \\
                & & \\
                & & \\
                & & \\
            \end{matrix}
            & \begin{matrix} \\ \\ \\ X \Sigma_p x^{\ast} \\ \\ \\ \end{matrix} \\
            \hdashline \\
            \begin{matrix} &  x^{\ast^{\top}}  \Sigma_p X^{\top} & \end{matrix} & \Sigma_p
        \end{array}
        \right]
        \right)
    = p(\mathbf{Y}, f^{\ast}| X, x^{\ast})
\end{gather}

The expression in \ref{def:predictive-dist} can then be derived using theorem \ref{thrm:Gaussian-Conditioning} on
conditioning of multivariate Gaussian's.

\subsection{Gaussian Process Regression}
By abandoning the parameters $\boldsymbol{\beta}$, in Gaussian process regression we are always
directly aiming for the predictive distribution of
$f^{\ast} := f(x^{\ast})$ at $x^{\ast}$ given our observations.

Let us first transform the Bayesian linear regression example from the last section
into a GP regression problem.

For the distribution of $\mathbf{F}_X = [f(x_1) \dots f(x_n))]$ with given $X = [x_1 \dots x_n]^{\top}$ we had:
\begin{gather*}
    \mathbf{F}_X \sim \N(0,  X \Sigma_p X^{\top})
\end{gather*}

Alternatively this can be written as a distribution over the function $f(x)$:

\begin{gather*}
    f(x) \sim GP(0, k(x, x'))
\end{gather*}
where $k(x,x')$ needs to be chosen such that for an input X we obtain $K_{XX} =  X \Sigma_p X^{\top}$.
For example if we assume $\Sigma_p = \sigma_p I$, we would choose $k(x,x') = \sigma_p x^{\top} x'$.

%Assuming $\Sigma_p = \sigma_p I$ and $\Sigma_n = \sigma_n I)$ we get for the kernel function:
%
%\begin{gather*}
%    k(x, x') = \sigma_p x^{\top} x' +  \mathbbm{1}_{x = x'}\sigma_n
%\end{gather*}

%δ pq is a Kronecker delta which is one iff p = q and zero otherwise.

Combining $f^{\ast}$ and $\mathbf{Y}$ into a single random vector we can use the theorem \ref{thrm:Gaussian-Conditioning}
to arrive at the same posterior predictive distribution
$p(f^{\ast} | \mathbf{Y}, X, x^{\ast})$ as presented in \ref{def:predictive-dist}.
The joint distribution of $f^{\ast}$ and $\mathbf{Y}$ can be expressed as follows:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        f^{\ast}
    \end{bmatrix} =
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        0
        \end{bmatrix},
        \begin{bmatrix}
        K_{XX} + \Sigma_n & K_{Xx^{\ast}} \\
        K_{x^{\ast}X} & K_{x^{\ast}x^{\ast}}
        \end{bmatrix}
        \right)
    = p(\mathbf{Y}, f^{\ast}| X, x^{\ast})
\end{gather}

where:
\begin{gather*}
    K_{XX} =
    \begin{bmatrix}
        k(x_1, x_1) & k(x_1, x_2) & \dots & k(x_1, x_n)\\
        \vdots  &  & \vdots  & \vdots \\
        k(x_n, x_1)  & k(x_n, x_1) & \dots  & k(x_n, x_n)
    \end{bmatrix}, \\
    K_{Xx^{\ast}} = K_{x^{\ast}X}^{\top} =
    \begin{bmatrix}
        k(x_1, x^{\ast}) \\
        \vdots \\
        k(x_n,  x^{\ast})
    \end{bmatrix} \text{ and }
    K_{x^{\ast}x^{\ast}} = k(x^{\ast}x^{\ast})
\end{gather*}

Unlike in chapter \ref{ch:time-series-decomposition-and-regression}, $f(x)$ is no longer assumed to be a
deterministic function. This way, GP regression allows us to treat $\mathbf{E}$ not simply as an error term but an actual part of
our signal which we can predict. If $\mathbf{E}$ is not independent noise but for example a time series, where
the elements of $\mathbf{E}$ are correlated, we want to leverage the information we have about an unobserved
time point given our observations.
Hence, we are not only interested in $p(f^{\ast} | \mathbf{Y}, X, x^{\ast})$ but also
in the predictive distribution $p(Y^{\ast} | \mathbf{Y}, X, x^{\ast})$ for $Y^{\ast} = Y(x^{\ast}) = f(x^{\ast}) + E(x^{\ast})$.

We had:
\begin{gather*}
    \mathbf{Y}|X \sim \N(0,  X \Sigma_p X^{\top} + \Sigma_n) \\
\end{gather*}

Alternatively we can assume a distribution over the function $Y(x)$:

\begin{gather*}
    Y(x) \sim GP(0, k(x, x'))
\end{gather*}
where $k(x,x')$ needs to be chosen such that for an input X we obtain $K_{XX} =  X \Sigma_p X^{\top} + \Sigma_n$

One can follow again the same procedure as before, by combining $Y^{\ast}$ and $\mathbf{Y}$ into a single random vector
and deriving the predictive distribution $p(Y^{\ast} | \mathbf{Y}, X, x^{\ast})$ by conditioning.

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        Y^{\ast}
    \end{bmatrix} =
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        0
        \end{bmatrix},
        \begin{bmatrix}
        K_{XX} & K_{Xx^{\ast}} \\
        K_{x^{\ast}X} & K_{x^{\ast}x^{\ast}}
        \end{bmatrix}
        \right)
    = p(\mathbf{Y}, f^{\ast}| X, x^{\ast})
\end{gather}





TODO
One could hence also assume additional measurement noise:
\begin{align*}
    Z(x) = Y(x) + \epsilon = f(x) + E(x) + \epsilon  && \epsilon \sim \N(0, \sigma_n^{2})
\end{align*}




TODO
This still  assumes that $\Sigma_n$ is known.



