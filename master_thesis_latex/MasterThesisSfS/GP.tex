\chapter{Gaussian Process Regression}\label{ch:gaussian-process-regression}


We again consider a regression problem of mapping the input $x$ to an output $f(x)$.
One approach as we have seen in seen in \ref{ch:time-series-decomposition-and-regression}
is to restrict ourselves to a
class of functions, which in our case has been the class of linear functions of the input.
Another approach is to assign a prior probability to every possible function, where higher
probabilities are assigned to functions that are assumed to be more likely.
Inference in the Bayesian framework is then based on the posterior distribution of these
functions given some potentially noisy observations of $f(x)$.

This chapter first provides a definition of a Gaussian Process and then describes how they
can be used to solve a regression problem.

\section{Gaussian Process Definition}

A Gaussian process can be viewed as a gaussian distribution over functions or as an infinite set of random
variables representing the values of the function $f(x)$ at location $x$.
The Gaussian process is thus a generalization of the Gaussian distribution and a formal definition is given
by \citeauthor{rasmussen_gaussian_2006} :

\begin{definition}[Gaussian Process]\label{def:GP}
 A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.
\end{definition}


As a (multivariate) Gaussian distribution is defined by its mean and covariance matrix a gaussian process (GP) is
uniquely identified by its mean $m(x)$ and covariance function $k(x,x')$.

We write

\begin{gather}
    f(x) \sim GP(m(x), k(x,x')), \\
\end{gather}
with
\begin{gather}
    m(x) = \mathbb{E}[f(x)] \\
    k(x,x') = \mathbb{E}[(f(x)-m(x))(f(x')-m(x'))]
\end{gather}

Where where $m$ is called the mean function and k is called covariance (kernel) function, which encodes some assumption
about correlation of the response variable at different inputs.

If we assume $X$ to be the index set or set of possible inputs of $f$, then there is a random variable
$F_x := f(x)$ such that for a set $A \subset X$ with $A={x_1, \dots x_n}$ it holds that:

\[F_A = [F_{x_0}, \dots , F_{x_n}] \sim \N(\mu_A,\,K_{AA})\]
for
\begin{gather}
    K_{AA} =
    \begin{bmatrix}
        k(x_1, x_1) & k(x_1, x_2) & \dots & k(x_1, x_n)\\
        \vdots  &  & \vdots  & \vdots \\
        k(x_n, x_1)  & k(x_n, x_1) & \dots  & k(x_n, x_n)
    \end{bmatrix} \text{and }
    \mu_A =
    \begin{bmatrix}
        \mu(x_1) \\
        \vdots \\
        \mu(x_n)
    \end{bmatrix}
\end{gather}
The finite marginals $F_{x_1}, \dots, F_{x_n}$ of the GP thus have a multivariate gaussian distribution.
In our running example we consider $X$ to be the time interval $T_0=[0, T]$ however it could be higher dimensional.

Note that Gaussian processes with finite index sets and hence with joint gaussian distribution is just a specific case
of Gaussian process. If we assume an ARMA process with gaussian innovations for the blood pressure time series
we can view the time series
as collection of normally distributed random variables and are thus dealing with a gaussian process.


If we consider the linear regression case from \ref{ch:time-series-decomposition-and-regression} and assume a
prior distribution
on the $\beta$'s, i.e. $\beta \sim N(0, I)$ then the predictive distribution over $\mu = X \beta$ is Gaussian:
\[
    \mu \sim \N(0, XX^{\top})
\]
and represents a GP with mean function $m(x) = 0$ and kernel function $k(x, x') = x^{\top}x'$.
This special case of gaussian process regression with this specific kernel function is known as baysian linear regression.


\section{Gaussian Process Regression}
Predictions in the Bayesian regression setting is finding the posterior distribution of  $f^{\ast} := f(x^{\ast})$ at $x^{\ast}$
given some potentially noisy observations of $f(x)$. This is made possible by employing a prior distribution
over the function $f(x)$. As we have seen before if we use a Gaussian process to do so, we can interpret this as
assuming a Gaussian distribution over $f(x)$.

\subsection{Bayesian Linear Regression}

If we consider again the linear regression model from chapter \ref{ch:time-series-decomposition-and-regression}, however
a more general setting is assumed where the data generating process does not need to be a time series process.
We denote the mean function with $f(x)$ instead of $\mu(t)$ and $Y_i$ is again a noisy observations of
$f(x_i)$, where the additive error $E_i$ does not necessarily need to arrive from a time series process $E(t)$.
We obtain the following data generating model:
\begin{align*}
    f(x_i) &= x_i^{\top}\beta, & Y_i &= f(x_i) + E_i,  & (i = 1, \dots n)
\end{align*}
with $x_i \in \mathbb{R}^p$ being again the input vector and $\beta \in \mathbb{R}^p$ is the vector with
the regression coefficients.

and in matrix from:
\begin{align*}
    \mathbf{Y} = X \beta + \mathbf{E}
\end{align*}
Where $\mathbf{Y} = [Y_{1}, \dots Y_{n}]^{\top}$ is the observed data,
$X = [x_{1}, \dots x_{n}]^{\top} \in \mathbb{R}^{n \times p}$ is the design matrix.
As before gaussian but potentially correlated errors $\mathbf{E} = [E_{1}, \dots E_{n}]^{\top}$
are assumed:
\begin{gather*}
    \mathbf{E} \sim \N(0, \Sigma_n)
\end{gather*}
If $\mathbf{E}$ is an ARMA process then this assumption is valid as long as the
innovations are always normally distributed.

The likelihood of the observations $Y$ given $X$ and $\beta$ is then:

\begin{gather*}
    p(Y|X,\beta) =
    = \frac{1}{( (2\pi)^{n/2} \sqrt {\det(\Sigma_n))}}
    \exp(-\frac{1}{2}(y - X\beta)^{\top} \Sigma_n^{-1}(y-X\beta))
    = \N(X \beta, \Sigma_n)
\end{gather*}

The Bayesian approach differs from the on in chapter \ref{ch:time-series-decomposition-and-regression} in
that we assume a prior distribution over the regression coefficients $\beta$, based on
what we believe are likely values for the coefficients.
To stay in the realm of gaussian processes the prior has to be guassian and we choose:

\begin{gather*}
    p(\beta) = \N(0, \Sigma_p)
\end{gather*}

Given our observations $Y$  we can use Bayes' theorem to calculate the posterior distribution over $\beta$:
\begin{gather*}
    p(\beta|y, X) = \frac{p(y,\beta|X)}{p(y|X)} = \frac{p(y|X,\beta)p(\beta)}{p(y|X)}
\end{gather*}

One approach is to just plug in the expressions for $p(y|X,\beta)$ and $p(\beta|y, X)$ from above, with:

\begin{gather*}
    p(y|X) = \int p(y|X,w) p(w) dw = \N(0, X \Sigma_p X^{\top} + \Sigma_n)
\end{gather*}

Or it can be helpful to combine the coefficients and the observations into a single random vector with
multivariate normal distribution:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        \beta
    \end{bmatrix}
    = \begin{bmatrix} X \\ I_p \end{bmatrix} \beta + \begin{bmatrix} I_n \\ 0 \end{bmatrix}  E
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        \vdots \\
        \vdots \\
        0 \\
        0 \\
        \vdots \\
        0
        \end{bmatrix},
        \left[
        \begin{array}{ c:c c c }
            \begin{matrix}
                & & \\
                & & \\
                & & \\
                & X \Sigma_p X^{\top} + \Sigma_n & \\
                & & \\
                & & \\
                & & \\
            \end{matrix}
            & \begin{matrix} \\ \\ \\ X \Sigma_p  \\ \\ \\ \end{matrix} \\
            \hdashline \\
            \begin{matrix} &  \Sigma_p X^{\top} & \end{matrix} & \Sigma_p
        \end{array}
        \right]
        \right)
    = p(\mathbf{Y}, \beta)
\end{gather}

with $\Sigma_p X^{\top} + \Sigma_n \in \mathbb{R}^{n\times n}$ and $\Sigma_p X^{\top} \in \mathbb{R}^{p\times n}$.

To find now the posterior distribution $p(\beta |  \mathbf{Y})$ one can use the well known rules for deriving conditional
distributions for jointly Gaussian random vectors, which will be presented quickly.
Let's say that $\mathbf{A} \sim \N(\mu_A, \Sigma_{AA})$ and $\mathbf{B} \sim \N(\mu_B, \Sigma_{BB})$ are
Gaussian random vectors with the following joint distribution:

\begin{gather}
    p(\mathbf{A}, \mathbf{B}) = \N(
    \begin{bmatrix}
        \mu_A \\
        \mu_B
    \end{bmatrix},
    \begin{bmatrix}
        \Sigma_{AA} & \Sigma_{AB} \\
        \Sigma_{BA} & \Sigma_{BB}
    \end{bmatrix}
\end{gather}
The distribution $p(\mathbf{B} | \mathbf{A}=a$ is also normally distributed with mean $\bar{\mu}$ and covariance
$\bar{\Sigma}$ of the following form:

\begin{align}\label{def:conditioning}
    \bar{\Sigma} = \Sigma_{B B} - \Sigma_{B A} \Sigma_{A A}^{-1} \Sigma_{A B} & & \bar{\mu} = \mu_{B} + \Sigma_{BA} \Sigma_{AA}^{-1}(a - \mu_A)
\end{align}


Using this rule we obtain for the mean and covariance of $p(\beta | \mathbf{Y}=y) \sim \N(\bar{\mu}, \bar{\Sigma})$:

\begin{align*}
    \bar{\Sigma} = \Sigma_{p} - \Sigma_p X^{\top}(X \Sigma_p X^{\top} + \Sigma_n)^{-1} X  \Sigma_p & & \bar{\mu} = \mu_{\beta} + \Sigma_p X^{\top}(X \Sigma_p X^{\top} + \Sigma_n)^{-1}y
\end{align*}

and using Woodbury matrix identity we can further simplify:
\begin{align*}
    \bar{\Sigma} = (X^{\top}\Sigma_n^{-1}X + \Sigma_p^{-1})^{-1} & & \bar{\mu} = \bar{\Sigma} X^{\top} \Sigma_n^{-1} y
\end{align*}

Deriving the predictive distribution of $f^{\ast} := f(x^{\ast})$ at $x^{\ast}$ given our
observations is then straight forward:

\begin{gather}\label{def:predictive-dist}
    p(f^{\ast} | \mathbf{Y}) = \N(x^{\ast^{\top}} \bar{\mu}, x^{\ast^{\top}} \bar{\Sigma} x^{\ast})
\end{gather}

One can also use the rules for conditioning to directly derive $f^{\ast} | \mathbf{Y}$.
Similarly to before we can write the joint distribution $p(\mathbf{Y}, f^{\ast})$:


\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        f^{\ast}
    \end{bmatrix}
    = \begin{bmatrix} X \\ x^{\ast} \end{bmatrix} \beta + \begin{bmatrix} I_n \\ 0 \end{bmatrix}  E
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        \vdots \\
        \vdots \\
        0 \\
        0
        \end{bmatrix},
        \left[
        \begin{array}{ c:c c c }
            \begin{matrix}
                & & \\
                & & \\
                & & \\
                & X \Sigma_p X^{\top} + \Sigma_n & \\
                & & \\
                & & \\
                & & \\
            \end{matrix}
            & \begin{matrix} \\ \\ \\ X \Sigma_p x^{\ast} \\ \\ \\ \end{matrix} \\
            \hdashline \\
            \begin{matrix} &  x^{\ast^{\top}}  \Sigma_p X^{\top} & \end{matrix} & \Sigma_p
        \end{array}
        \right]
        \right)
    = p(\mathbf{Y}, \beta)
\end{gather}

The expression in \ref{def:predictive-dist} can then be derived similarly to before using the rules for
conditioning of gaussian random vectors described in \ref{def:conditioning}.

\subsection{Gaussian Process Regression}
By abandoning the parameters $\beta$, in Gaussian process regression we are always
directly aiming for the predictive distribution of
$f^{\ast} := f(x^{\ast})$ at $x^{\ast}$ given our observations.
In order to derive $f^{\ast}|\mathbf{Y}$
we will again consider the Bayesian linear regression example from the last section.

We chose a gaussian prior over the regression coefficients and for the prior distribution $p(\mathbf{Y}$) we had:

\begin{gather*}
    \mathbf{Y} \sim \N(0,  \Sigma_p X^{\top} + \Sigma_n)
\end{gather*}

Alternatively we can assume a distribution over $\mathbf{Y}$ as a function of x:

\begin{gather*}
    \mathbf{Y}(x) \sim GP(0, k(x, x'))
\end{gather*}
where $k(x,x')$ needs to be chosen such that for an input X we obtain $K_{XX} =  \Sigma_p X^{\top} + \Sigma_n$


to obtain $y^{\ast}|\mathbf{Y}$

We have seen that the likelihood of the observations $Y$ given $X$ and $\beta$ is given by:
\begin{gather*}
    \mathbf{Y}|X,\beta \sim \N(X \beta, \Sigma_n)
\end{gather*}

If we assume now

If we assume $\mathbf{Y}$ as a random function of $x$ we can write:

\begin{gather*}
    \mathbf{Y}(x) \sim GP(X \beta, K_{YY})
\end{gather*}

where $K_{YY} needs to be $



