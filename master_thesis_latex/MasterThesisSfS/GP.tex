
\chapter{Gaussian Process Regression}\label{ch:gaussian-process-regression}


We again consider a regression problem of mapping the input $x$ to an output $f(x)$.
One approach as we have seen in seen in \ref{ch:time-series-decomposition-and-regression}
is to restrict ourselves to a
class of functions, which in our case has been the class of linear functions of the input.
Another approach is to assign a prior probability to every possible function, where higher
probabilities are assigned to functions that are assumed to be more likely.
A Gaussian process can be viewed as a gaussian distribution over functions or as an infinite set of random
variables representing the values of the function $f(x)$ at location $x$.
The Gaussian process is thus a generalization of the Gaussian distribution and a formal definition is given
by \citeauthor{rasmussen_gaussian_2006} :

\begin{definition}[Gaussian Process]\label{def:GP}
 A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.
\end{definition}


As a (multivariate) Gaussian distribution is defined by its mean and covariance matrix a gaussian process (GP) is
uniquely identified by its mean $m(x)$ and covariance function $k(x,x')$.

We write

\begin{gather}
    f(x) \sim GP(m(x), k(x,x')), \\
\end{gather}
with
\begin{gather}
    m(x) = \mathbb{E}[f(x)] \\
    k(x,x') = \mathbb{E}[(f(x)-m(x))(f(x')-m(x'))]
\end{gather}

Where where $m$ is called the mean function and k is called covariance (kernel) function, which encodes some assumption
about correlation of the response variable at different inputs.

If we assume $X$ to be the index set or set of possible inputs of $f$, then there is a random variable
$F_x := f(x)$ such that for a set $A \subset X$ with $A={x_0, \dots x_n}$ it holds that:

\[F_A = [F_{x_0}, \dots , F_{x_n}] \sim \N(m_A,\,K_{AA})\]
for
\begin{gather}
    K_{AA} =
    \begin{bmatrix}
        k(x_0, x_1) & k(x_1, x_2) & \dots & k(x_1, x_n)\\
        \vdots  &  & \vdots  & \vdots \\
        k(x_n, x_1)  & k(x_n, x_1) & \dots  & k(x_n, x_n)
    \end{bmatrix} \text{and }
    \mu_A =
    \begin{bmatrix}
        m(x_1) \\
        \vdots \\
        \mu(x_n)
    \end{bmatrix}
\end{gather}
The finite marginals $F_{x_0}, \dots, F_{x_n}$ of the GP thus have a multivariate gaussian distribution.
In our running example we consider $X$ to be the time interval $T_0=[0, T]$ however it could be higher dimensional.

Note that Gaussian processes with finite index sets and hence with joint gaussian distribution is just a specific case
of Gaussian process. If we assume an ARMA process with gaussian innovations for the blood pressure time series
we can view the time series
as collection of normally distributed random variables and are thus dealing with a gaussian process.


If we consider the linear regression case from \ref{ch:time-series-decomposition-and-regression} and assume a
prior distribution
on the $\beta$'s, i.e. $\beta \sim N(0, I)$ then the predictive distribution over $\mu = X \beta$ is Gaussian:
\[
    \mu \sim \N(0, XX^{\top})
\]
and represents a GP with mean function $m(x) = 0$ and kernel function $k(x, x') = x^{\top}x'$.
This special case of gaussian process regression with this specific kernel function is known as baysian linear regression.


\section{Bayesian Linear Regression}

If we consider again the linear regression model from chapter \ref{ch:time-series-decomposition-and-regression} but
we denote the mean function with $f(x)$ instead of $\mu(t)$:
\begin{align*}
    f(x) &= x^{\top}\beta, & y &= f(x) + E, & E &\sim \N(0, \Sigma_n)
\end{align*}
with $x \in \mathbb{R}^p$ being again the input vector and $\beta \in \mathbb{R}^p$ is the vector with
the regression coefficients.

Assuming observations $y = [y_1 \dots y_n]^{\top}$ for inputs $X = [x_1 \dots x_n]^{\top}$
we can calculate the likelihood,
which is the probability density of the observations given $X$ and $\beta$:

\begin{gather*}
    p(y|X,\beta) =
    = \frac{1}{( (2\pi)^{n/2} \sqrt {\det(\Sigma_n))}}
    \exp(-\frac{1}{2}(y - X\beta)^{\top} \Sigma_n^{-1}(y-X\beta))
    = \N(X \beta, \Sigma_n)
\end{gather*}

Unlike in chapter \ref{ch:time-series-decomposition-and-regression} we will now also
assume a prior distribution over the regression coefficients $\beta$, based on
what we believe is are likely values for the coefficients. We will choose a gaussian
distribution with mean $0$ and covariance $\Sigma_p$, i.e.:

\begin{gather*}
    p(\beta) = \N(0, \Sigma_p)
\end{gather*}

Given our observations $y = [y_1 \dots y_n]^{\top}$  we can use Bayes' theorem to calculate the posterior distribution over the $\beta$s:
\begin{gather*}
    p(\beta|y, X) = \frac{p(y,\beta|X)}{p(y|X)} = \frac{p(y|X,\beta)p(\beta)}{p(y|X)}
\end{gather*}

One approach is to just plug in the expressions for $p(y|X,\beta)$ and $p(\beta|y, X)$ from above, with:

\begin{gather*}
    p(y|X) = \int p(y|X,w) p(w) dw
\end{gather*}

Or it can be helpful to combine the coefficients and the observations into a single random vector with
multivariate normal distribution:

\begin{gather}
    \begin{bmatrix}
        y_1 \\
        \vdots \\
        y_n \\
        \beta_1 \\
        \vdots \\
        \beta_p
    \end{bmatrix}
    = \begin{bmatrix} X \\ I_p \end{bmatrix} \beta + E
    \sim \N \left(
        \begin{bmatrix}
        \mu_1 \\
        \vdots \\
        \mu_{y_n} \\
        \mu_{\beta_1} \\
        \vdots \\
        \mu_{\beta_{p}}
        \end{bmatrix},
        \begin{bmatrix}
            \Sigma_{yy} & \begin{matrix} \\ \Sigma_{y\beta} \\  \\ \end{matrix} \\
            \begin{matrix} & \Sigma_{\beta y} & \end{matrix} & \Sigma_{\beta \beta}
        \end{bmatrix}
        \right)
    = p(y, \beta)
\end{gather}

with $\Sigma_{yy} \in \mathbb{R}^{n\times n}$, $\Sigma_{\beta y} \in \mathbb{R}^{p\times n}$,
$\Sigma{\beta \beta} \in \mathbb{R}^{p\times p}$, $\Sigma_{y\beta} = \Sigma_{\beta y}^{\top}$.

Since $E &\sim \N(0, \Sigma_n)$ and $ \beta \sim \N(0, \Sigma_p)$ we have:

\begin{align*}
   \Sigma_{yy} = X \Sigma_p X^{\top} + \Sigma_n  & \Sigma_{\beta y} = \Sigma_p X^{\top} & \Sigma{\beta \beta} = \Sigma_p \\
   \mu_{y_0} = \dots = \mu_{y_n} = \mu_{\beta_0} =  \dots = \mu_{\beta_{p-1}} = 0
\end{align*}

One well known properties of multivariate normal distributions is that the conditional distribution of
$\beta$ given $y$ is also normal and has the following mean $\bar{\mu}$ and covariance $\bar{\Sigma}$:

\begin{gather}
    p(\beta | y X) = \N(\bar{\mu}, \bar{\Sigma}) \\
\end{gather}
with:
\begin{gather}
    \bar{\Sigma} = \Sigma_{\beta \beta} - \Sigma_{\beta y} \Sigma_{y y}^{-1} \Sigma_{y \beta} \\
    \bar{\mu} = \mu_{\beta} -
\end{gather}

%    \sim
%
%    N \left(
%    \begin{bmatrix}
%        0 \\
%        \vdots \\
%        0 \\
%        0 \\
%        \vdots \\
%        0
%    \end{bmatrix},
%
%    \begin{bmatrix}
%        \Sigma_{yy} & \begin{matrix} \\ \Sigma_{yw} \\  \\ \end{matrix} \\
%        \begin{matrix} & \Sigma_{wy} & \end{matrix} & \Sigma_{ww}
%    \end{bmatrix}
%
%
%    \right) = p(y, \beta)
%\end{gather}


%$\begin{bNiceArray}{cw{c}{1cm}c|c}[margin]
%\Block{3-3}{A} & & & 0 \\
%& & & \Vdots \\
%& & & 0 \\
%\hline
%0 & \Cdots& 0 & 0
%\end{bNiceArray}$
%
%
%

