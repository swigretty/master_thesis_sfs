\usepackage{mathtools}
\usepackage{amsmath}\chapter{Gaussian Process Regression}\label{ch:gaussian-process-regression}

We again consider a regression problem of mapping the input $x$ to an output $f(x)$.
In order to solve such a problem one usually needs some additional constraints on the $f(x)$.
In \ref{ch:time-series-decomposition-and-regression} we restricted ourselves to the class of linear functions.
Another approach is to assign a prior probability to every possible function, where higher
probabilities are assigned to functions that are assumed to be more likely.
Inference in this Bayesian setting is then based on the posterior distribution of these
functions given some potentially noisy observations of $f(x)$.

This chapter first provides a definition of a Gaussian Process and then describes how it
can be used to solve a regression problem.
The argument presented in this chapter is based on the textbook of \citeauthor{rasmussen_gaussian_2006}.

\section{Gaussian Process Definition}\label{sec:gaussian-process-definition}

A Gaussian process (GP) can be viewed as a gaussian distribution over functions or as an infinite set of random
variables representing the values of the function $f(x)$ at location $x$.
The Gaussian process is thus a generalization of the Gaussian distribution and a formal definition is given
by \citeauthor{rasmussen_gaussian_2006} :

\begin{definition}[Gaussian Process]\label{def:GP}
 A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.
\end{definition}


As a (multivariate) Gaussian distribution is defined by its mean and covariance matrix, a GP is
uniquely identified by its mean $m(x)$ and covariance (kernel) function $k(x,x')$.

We write

\begin{gather*}
    f(x) \sim GP(m(x), k(x,x'))
\end{gather*}
with
\begin{gather*}
    m(x) = \mathbb{E}[f(x)] \\
    k(x,x') = \mathbb{E}[(f(x)-m(x))(f(x')-m(x'))]
\end{gather*}

If we assume $X$ to be the index set or set of possible inputs of $f$, then there is a random variable
$F_x := f(x)$ such that for a set $A \subset X$ with $A={x_1, \dots x_n}$ it holds that:

\[F_A = [F_{x_1}, \dots , F_{x_n}] \sim \N(\mu_A,\,K_{AA})\]
for
\begin{gather}\label{def:Kernel-Matrix}
    K_{AA} =
    \begin{bmatrix}
        k(x_1, x_1) & k(x_1, x_2) & \dots & k(x_1, x_n)\\
        \vdots  &  & \vdots  & \vdots \\
        k(x_n, x_1)  & k(x_n, x_1) & \dots  & k(x_n, x_n)
    \end{bmatrix} \text{and }
    \mu_A =
    \begin{bmatrix}
        \mu(x_1) \\
        \vdots \\
        \mu(x_n)
    \end{bmatrix}
\end{gather}

The finite marginals $F_{x_1}, \dots, F_{x_n}$ of the GP thus have a multivariate gaussian distribution.
In our running example we might consider $X$ to be the time interval $T_0=[0, T]$ however it could be higher dimensional.

Note that a GP with finite index set and hence with joint gaussian distribution is just a specific case
of GP. If we assume an ARMA process with gaussian innovations for the blood pressure time series, one can view the time series
as a collection of multivariate normally distributed random variables and thus as a GP.


If we consider the linear regression case from chapter \ref{ch:time-series-decomposition-and-regression} and assume a
prior distribution
on $\beta$, i.e. $\beta \sim N(0, I)$ then the predictive distribution over $\mu = X \beta$ is Gaussian:
\[
    \mu \sim \N(0, XX^{\top})
\]
This is equivalent to a GP with mean function $m(x) = 0$ and kernel function $k(x, x') = x^{\top}x'$.
This special case of gaussian process regression with this specific kernel function is known as Bayesian linear regression
and will be presented in the next section.


\section{Bayesian Linear Regression}\label{subsec:bayesian-linear-regression}
Predictions in the Bayesian regression setting is finding the posterior distribution of
$f^{\ast} := f(x^{\ast})$ at some input $x^{\ast}$, given some potentially noisy observations of $f(x)$.
This is made possible by employing a prior distribution
over the function $f(x)$.
As shown in section \ref{sec:gaussian-process-definition}, a GP is essentially
assuming a Gaussian distribution over functions.
This section however still stays in the domain of parametric models,
in which case we assume a distribution over the parameters of the function $f(x)$,
rather than over the function itself.
In Bayesian linear regression we are thus assuming a distribution over the
regression coefficients $\beta$.

Recall the linear regression model from chapter \ref{ch:time-series-decomposition-and-regression}.
However, we are assuming a more general setting, where the data generating process does not need to be a time series process.
The function is denoted with $f(x)$ instead of $\mu(t)$ and $Y_i$ is again a noisy observation of
$f(x_i)$, where the additive error $E_i$ does not necessarily need to be from a time series process $(E_t: t \in \{t_1, t_2, \dots  t_n \})$.
We obtain the following data generating model:
\begin{align*}
    f(x_i) &= x_i^{\top}\beta, & Y_i &= f(x_i) + E_i,  & (i = 1, \dots n)
\end{align*}
with $x_i \in \mathbb{R}^p$ being again the input vector and $\beta \in \mathbb{R}^p$ is the vector with
the regression coefficients.

In matrix from:
\begin{align*}
    \mathbf{Y} = X \beta + \mathbf{E}
\end{align*}
Where $\mathbf{Y} = [Y_{1}, \dots Y_{n}]^{\top}$ is the observed data,
$X = [x_{1}, \dots x_{n}]^{\top} \in \mathbb{R}^{n \times p}$ is the design matrix.
We assume again gaussian but potentially correlated errors $\mathbf{E} = [E_{1}, \dots E_{n}]^{\top}$:
\begin{gather*}
    \mathbf{E} \sim \N(0, \Sigma_e)
\end{gather*}
If $\mathbf{E}$ is an ARMA process, then every element of the time series $E_{i}$
is itself a sum of innovations.
Therefore, $\mathbf{E}$ is gaussian as long as it has gaussian innovations.

The likelihood, i.e. the probability of the observations $\mathbf{Y}$ given $X$ and $\beta$ is then:

\begin{gather*}
    p(\mathbf{Y}|X,\beta) =
    = \frac{1}{(2\pi)^{n/2} \sqrt {\det(\Sigma_e)}}
    \exp(-\frac{1}{2}(y - X\beta)^{\top} \Sigma_e^{-1}(y-X\beta))
    = \N(X \beta, \Sigma_e)
\end{gather*}

Until now the regression model is exactly the same as in chapter \ref{ch:time-series-decomposition-and-regression}.
The Bayesian approach is different in that we additionally assume a prior distribution over the
regression coefficients $\beta$, based on what we believe are likely values for the coefficients.
To stay in the realm of gaussian processes the prior has to be Gaussian and we choose:

\begin{gather*}
    p(\beta) = \N(0, \Sigma_p)
\end{gather*}
Note how the function $f(x_i)=x_i^{\top}\beta$ is now no longer deterministic but a random function.

Given our observations $\mathbf{Y}$  we can use Bayes' theorem to calculate the posterior distribution over $\beta$:
\begin{gather*}
    p(\beta| \mathbf{Y}, X) = \frac{p(\mathbf{Y},\beta|X)}{p(\mathbf{Y}|X)} =
    \frac{p(\mathbf{Y}|X,\beta)p(\beta)}{p(\mathbf{Y}|X)}
\end{gather*}

One approach is to just plug in the expressions for
$p(\mathbf{Y}|X,\beta)$ and $p(\beta|\mathbf{Y}, X)$ from above, with the marginal likelihood:

\begin{gather*}
    p(\mathbf{Y}|X) = \int p(\mathbf{Y}|X,\beta) p(\beta) d\beta = \N(0, X \Sigma_p X^{\top} + \Sigma_e)
\end{gather*}

Or it can be helpful to combine the coefficients and the observations into a single random vector with
multivariate normal distribution:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        \beta
    \end{bmatrix}
    = \begin{bmatrix} X \\ I_p \end{bmatrix} \beta + \begin{bmatrix} I_n \\ 0 \end{bmatrix}  \mathbf{E}
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        \vdots \\
        \vdots \\
        0 \\
        0 \\
        \vdots \\
        0
        \end{bmatrix},
        \left[
        \begin{array}{ c:c c c }
            \begin{matrix}
                & & \\
                & & \\
                & & \\
                & X \Sigma_p X^{\top} + \Sigma_e & \\
                & & \\
                & & \\
                & & \\
            \end{matrix}
            & \begin{matrix} \\ \\ \\ X \Sigma_p  \\ \\ \\ \end{matrix} \\
            \hdashline \\
            \begin{matrix} &  \Sigma_p X^{\top} & \end{matrix} & \Sigma_p
        \end{array}
        \right]
        \right)
    = p(\mathbf{Y}, \beta | X)
\end{gather}

with $\Sigma_p X^{\top} + \Sigma_e \in \mathbb{R}^{n\times n}$ and $\Sigma_p X^{\top} \in \mathbb{R}^{p\times n}$.

To find now the posterior distribution $p(\beta | \mathbf{Y}, X)$ one can use the rules for deriving conditional
distributions for multivariate Gaussian's presented in theorem \ref{thrm:Gaussian-Conditioning}.

\begin{theorem}\label{thrm:Gaussian-Conditioning} (\citeauthor{von_mises_mathematical_1964})

Let $A \sim \N(\mu_A, \Sigma_{AA})$ and $B \sim \N(\mu_B, \Sigma_{BB})$ be
Gaussian random vectors with the following joint distribution:

\begin{gather*}
    p(A, B) = \N \left(
    \begin{bmatrix}
        \mu_A \\
        \mu_B
    \end{bmatrix},
    \begin{bmatrix}
        \Sigma_{AA} & \Sigma_{AB} \\
        \Sigma_{BA} & \Sigma_{BB}
    \end{bmatrix}
    \right)
\end{gather*}

Then the conditional distribution $p(\mathbf{B} | \mathbf{A}=a)$ is also normally distributed
with mean $\bar{\mu}$ and covariance $\bar{\Sigma}$ of the following form:

\begin{align*}
    \bar{\Sigma} = \Sigma_{B B} - \Sigma_{B A} \Sigma_{A A}^{-1} \Sigma_{A B} & & \bar{\mu} = \mu_{B} + \Sigma_{BA} \Sigma_{AA}^{-1}(a - \mu_A)
\end{align*}


\end{theorem}



Using theorem \ref{thrm:Gaussian-Conditioning} the posterior distribution over $\beta$ is then given by:
\begin{gather*}
    p(\beta | \mathbf{Y}=y, X) \sim \N(\bar{\mu}, \bar{\Sigma}), \\
    \bar{\Sigma} = \Sigma_{p} - \Sigma_p X^{\top}(X \Sigma_p X^{\top} + \Sigma_e)^{-1} X  \Sigma_p, \\
    \bar{\mu} = \mu_{\beta} + \Sigma_p X^{\top}(X \Sigma_p X^{\top} + \Sigma_e)^{-1}y
\end{gather*}

The expression for the posterior mean and covariance matrix can be further simplified using Woodbury matrix identity
and we obtain:
\begin{align}\label{def:conditional-mean-var}
    \bar{\Sigma} = (X^{\top}\Sigma_e^{-1}X + \Sigma_p^{-1})^{-1} & & \bar{\mu} = \bar{\Sigma} X^{\top} \Sigma_e^{-1} y
\end{align}

Since $f(x) = x^{\top}\beta$, one can use the posterior mean and covariance matrix from
\ref{def:conditional-mean-var} to obtain the predictive distribution of $f^{\ast} := f(x^{\ast})$ at $x^{\ast}$
given our observations:
\begin{align}\label{def:predictive-dist}
    p(f^{\ast} | \mathbf{Y}, X, x^{\ast}) = \N(x^{\ast^{\top}} \bar{\mu}, x^{\ast^{\top}} \bar{\Sigma} x^{\ast})
\end{align}

One can also use the rules for conditioning to directly derive $f^{\ast} | \mathbf{Y}, X, x^{\ast}$.
Similar to before we can write the joint distribution $p(\mathbf{Y}, f^{\ast}| X, x^{\ast})$:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        f^{\ast}
    \end{bmatrix}
    = \begin{bmatrix} X \\ x^{\ast} \end{bmatrix} \beta + \begin{bmatrix} I_n \\ 0 \end{bmatrix}  \mathbf{E}
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        \vdots \\
        \vdots \\
        0 \\
        0
        \end{bmatrix},
        \left[
        \begin{array}{ c:c c c }
            \begin{matrix}
                & & \\
                & & \\
                & & \\
                & X \Sigma_p X^{\top} + \Sigma_e & \\
                & & \\
                & & \\
                & & \\
            \end{matrix}
            & \begin{matrix} \\ \\ \\ X \Sigma_p x^{\ast} \\ \\ \\ \end{matrix} \\
            \hdashline \\
            \begin{matrix} &  x^{\ast^{\top}}  \Sigma_p X^{\top} & \end{matrix} & \Sigma_p
        \end{array}
        \right]
        \right)
    = p(\mathbf{Y}, f^{\ast}| X, x^{\ast})
\end{gather}

The expression in \ref{def:predictive-dist} can then be derived using theorem \ref{thrm:Gaussian-Conditioning} on
conditioning of multivariate Gaussian's.

The next section will extend the Bayesian approach to non-parametric models and illustrate how Bayesian linear
regression is just a special case of GP regression.

\section{Bayesian Linear Regression as Gaussian Process Regression}\label{sec:gaussian-process-regression}
The linear model discussed so far, with a cyclic component represented by a cosine and a linear trend component,
might be an evident first guess.
However, it is unlikely that the BP values are exactly following this pattern.
Instead of reducing the function space to this specific class of linear functions, we may use our domain knowledge
to tell which functions of the infinite space of all functions are more likely to have generated our data.
As these functions are not characterized with explicit sets of
parameters, this approach belongs to the branch of non-parametric modelling.
By abandoning the parameters $\beta$, Gaussian process regression
directly aims for the predictive distribution of $f^{\ast} := f(x^{\ast})$ at an input $x^{\ast}$ given our observations.

Starting with the Bayesian linear regression example from last section and transforming it into a GP regression
problem, we recall that the distribution of $F_X = [f(x_1) \dots f(x_n))]^{\top}$ with given $X = [x_1 \dots x_n]^{\top}$ is:
\begin{gather*}
    F_X \sim \N(0,  X \Sigma_p X^{\top})
\end{gather*}

Alternatively this can be written as a distribution over the function $f(x)$:

\begin{gather*}
    f(x) \sim GP(0, k(x, x'))
\end{gather*}
where $k(x,x')$ needs to be chosen such that for an input X we obtain $K_{XX} =  X \Sigma_p X^{\top}$.
Given $\Sigma_p = \sigma_p I$, we would choose $k(x,x') = \sigma_p x^{\top} x'$, with the
input pairs $x$ and $x'$ only entering as a dot product.


%Note that since $\Sigma_p$ is postive definite we can define $\Sigma_p^{\frac{1}{2}} = (\Sigma_p^{\frac{1}{2}})^2=\Sigma_p$.
%Then defining $\phi(x) = \Sigma_p^{\frac{1}{2}} x$ the kernel function becomes $k(x, x') = \phi(x)^{\top} \phi(x')$,
%which is again the dot product of pairs of $\phi(x)$.
%This shows that Baysian linear regression with transformed inputs $\phi(x)$ and prior covariance matrix $\Sigma_p = I$,
%has the same effect as choosing a more complicated $\Sigma_p$ and leaving the inputs untouched.


%For example if we assume $\Sigma_p = \sigma_p I$, we would choose $k(x,x') = \sigma_p x^{\top} x'$.

%Assuming $\Sigma_p = \sigma_p I$ and $\Sigma_e = \Sigma_e I)$ we get for the kernel function:
%
%\begin{gather*}
%    k(x, x') = \sigma_p x^{\top} x' +  \mathbbm{1}_{x = x'}\Sigma_e
%\end{gather*}

%δ pq is a Kronecker delta which is one iff p = q and zero otherwise.

Combining $f^{\ast}$ and $\mathbf{Y}$ into a single random vector we can use the theorem \ref{thrm:Gaussian-Conditioning}
to arrive at the same posterior predictive distribution
$p(f^{\ast} | \mathbf{Y}, X, x^{\ast})$ as presented in \ref{def:predictive-dist}.
The joint distribution of $f^{\ast}$ and $\mathbf{Y}$ can be expressed as follows:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        f^{\ast}
    \end{bmatrix} =
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        0
        \end{bmatrix},
        \begin{bmatrix}
        K_{XX} + \Sigma_e & K_{Xx^{\ast}} \\
        K_{x^{\ast}X} & K_{x^{\ast}x^{\ast}}
        \end{bmatrix}
        \right)
    = p(\mathbf{Y}, f^{\ast}| X, x^{\ast})
\end{gather}

where:
\begin{gather*}
    K_{XX} =
    \begin{bmatrix}
        k(x_1, x_1) & k(x_1, x_2) & \dots & k(x_1, x_n)\\
        \vdots  &  & \vdots  & \vdots \\
        k(x_n, x_1)  & k(x_n, x_1) & \dots  & k(x_n, x_n)
    \end{bmatrix}, \\
    K_{Xx^{\ast}} = K_{x^{\ast}X}^{\top} =
    \begin{bmatrix}
        k(x_1, x^{\ast}) \\
        \vdots \\
        k(x_n,  x^{\ast})
    \end{bmatrix} \text{ and }
    K_{x^{\ast}x^{\ast}} = k(x^{\ast}x^{\ast})
\end{gather*}

\subsection{Time Series Gaussian Process Regression}

Unlike in chapter \ref{ch:time-series-decomposition-and-regression}, $f(x)$ is no longer assumed to be a
deterministic and parametric function.
This way, GP regression allows us to treat $\mathbf{E}$ not simply as an error
term but an actual part of our signal which we can predict. If $\mathbf{E}$ is not independent noise but for example a
time series, where the elements of $\mathbf{E}$ are correlated, we want to leverage the information we have about an
unobserved time point given our observations.
Hence, we are not interested in the posterior distribution of $f^{\ast}$ only, but also of
$Y^{\ast} := Y(x^{\ast}) = f(x^{\ast}) + E(x^{\ast})$.

Recall the expression for the marginal likelihood $p(\mathbf{Y}| X)$:
\begin{gather*}
    \mathbf{Y}|X \sim \N(0,  X \Sigma_p X^{\top} + \Sigma_e) \\
\end{gather*}

Alternatively, this can be expressed as a distribution over the function $Y(x)$:
\begin{gather*}
    Y(x) \sim GP(0, k(x, x'))
\end{gather*}
The kernel function $k(x,x')$ needs to be chosen such that for an index set X we obtain $K_{XX} =  X \Sigma_p X^{\top} + \Sigma_e$.
One can then follow again the same procedure as before and combine $Y^{\ast}$ and $\mathbf{Y}$ into a single random vector:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        Y^{\ast}
    \end{bmatrix}
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        0
        \end{bmatrix},
        \begin{bmatrix}
        K_{XX} & K_{Xx^{\ast}} \\
        K_{x^{\ast}X} & K_{x^{\ast}x^{\ast}}
        \end{bmatrix}
        \right)
    = p(\mathbf{Y}, Y^{\ast}| X, x^{\ast})
\end{gather}

The predictive distribution $p(Y^{\ast} | \mathbf{Y}, X, x^{\ast})$ is then again derived by conditioning.

One could also assume additional measurement noise on the time series $f(x) + E(x)$.
We then have for the observed time series $Y(x)$:
\begin{align*}
    Y(x) = f(x) + E(x) + \epsilon   && \epsilon \sim \N(0, \sigma_n^{2})
\end{align*}
To be inline with the literature on Gaussian process regression, we will from now on consider
our goal to find some function $f(x)$, which is a combination of the mean function, until now denoted by $f(x)$,
and the stationary time series $E(x)$.
The observed time series $Y(x)$ will thus be equivalent to $f(x)$ up to some additive independent noise term $\epsilon$.
We can write:
\begin{align*}
    Y(x) = f(x) + \epsilon && \epsilon \sim \N(0, \sigma_n^{2})
\end{align*}

Assuming the same linear model as before, we have for $F_X = [f(x_1), \dots f(x_n)]^{\top}$:
\begin{gather*}
    F_X = X \beta + \mathbf{E}, \text{ with $\beta \sim \N(0, \Sigma_p)$ and $\mathbf{E} \sim \N(0, \Sigma_e)$}
\end{gather*}
%with $\mathbf{E} = [E(x_1) \dots E(x_n)]^{\top} \sim \N(0, \Sigma_e)$ for some input $x_1 \dots x_n$.
%
%For some inputs $(x_i: i = 1 \dots n)$ we assume:
%\begin{gather*}
%    f(x_i) = x_i^{\top}\beta + E(x_i) \\
%    \text{with $\beta \sim \N(0, \Sigma_p)$} \\
%    \text{and  $\mathbf{E} = [E(x_1) \dots E(x_n)]^{\top} \sim \N(0, \Sigma_e$),}
%\end{gather*}

%For some inputs $(x_i: i = 1 \dots n)$ we assume:
%\begin{gather*}
%    f(x_i) = x_i^{\top}\beta + E(x_i) \\
%    \text{with $\beta \sim \N(0, \Sigma_p)$ and  $\mathbf{E} = [E(x_1) \dots E(x_n)]^{\top} \sim \N(0, \Sigma_e$),}
%\end{gather*}
Analogously we can write:
\begin{gather*}
    f(x) \sim GP(0, k(x, x')),
\end{gather*}
with $k(x,x')$ such that for an input $X = [x_1 \dots x_n]^{\top}$ we obtain $K_{XX} =  X \Sigma_p X^{\top} + \Sigma_e$.
Similarly $Y(x) \sim GP(0, k(x, x') + \delta_{x,x'}\sigma_n)$,
where $\delta_{x,x'}$ is the Kronecker delta which is one if $x = x'$ and zero otherwise.

The joint distribution of $\mathbf{Y}$ and $f^{\ast} := f(x^{\ast})$ is given by:
\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        f^{\ast}
    \end{bmatrix}
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        0
        \end{bmatrix},
        \begin{bmatrix}
        K_{XX} + \sigma_n^2 I & K_{Xx^{\ast}} \\
        K_{x^{\ast}X} & K_{x^{\ast}x^{\ast}}
        \end{bmatrix}
        \right)
    = p(\mathbf{Y}, f^{\ast}| X, x^{\ast})
\end{gather}


Also note how until now we have still assumed $\Sigma_e$, the covariance matrix of $\mathbf{E}$, to be known.
However, deriving $\Sigma_e$ for an ARMA process with irregularly spaced samples is not straight forward, as has already
been shown in chapter \ref{ch:time-series-decomposition-and-regression}.
The next section will illustrate how choosing a specific kernel function solves this problem.

\section{Mean Function}\label{subsec:mean-function}

Incorporating Explicit Basis Functions \citeauthor{rasmussen_gaussian_2006} p.27

Assuming we want to model $Y(x) = f(x) + \epsilon$, with $f(x) = m(x) + E(x)$ where $m(x)$ is a deterministic mean function
and the $E(x)$ is a time series process and $\epsilon$ is some iid. gaussian noise term with variance $\sigma_n^{2}$.
%In matrix notation for an input $X$:
%
%$$ \mathbf{Y} = \matthbf{M} + \mathbf{E} + \mathbf{\epsilon}$$

Then we can model $f(x)$ with a GP:
$$ f(x) \sim GP(m(x), k(x,x'))$$
By using the conditioning rule we arrive at the following predictive distribution for $f^{\ast} := f(x^{\ast})$:
\begin{gather*}
    p(f^{\ast}| \mathbf{Y} = y, X, x^{\ast}) = N(\bar{\mu}, \bar{\Sigma}) \\
    \text{with:} \\
    \bar{\mu} = m(x^{\ast}) + K_{x^{\ast}X} (K_{XX} + \sigma^{n} I )^{-1}(y - m(x))
    \text{where the predictive variance $\bar{\Sigma}$ is not affected by $m(x)$:}
    \bar{\Sigma} = K_{x^{\ast}x^{\ast}} - K_{x^{\ast}X} (K_{XX} + \sigma^{n} I )^{-1} K(X, x^{\ast})
\end{gather*}
Hence one could just fit a GP to $f(x) - m(x) = E(x)$:
$$ E(x) = f(x) - m(x) \sim GP(0, k(x,x'))$$ and add $m(x)$ to the predictive mean of $E^{\ast} = E(x^{\ast})$ to
arrive again at the predictive mean $\bar{\mu}$ of $f^{\ast}$ from above.



Regardless on how we derive the deteministic function $m(x)$ this will not have an impact on

\section{Kernel Functions}\label{subsec:kernel}

In the last section we started of with a describing the prior distribution over
$\mathbf{Y}$ or $F_X = [f(x_1) \dots f(x_n))]^{\top}$ and shoved that a kernel function $k(x, x')$ needs to be
chosen to match this distribution.
However, in Gaussian process regression it generally goes the other way around.
One would choose a prior distribution over $f(x)$ or $Y(x)$ first, which boils down to choosing a kernel function.
The kernel function evaluated at your inputs $X=[x_1 \dots x_n]^{\top}$ is then needed to calculate the
predictive distribution of $f^{\ast}$ or $y^{\ast}$.

The choice of kernel function depends on the assumptions about correlation in your output given arbitrary input pairs
$x$ and $x'$.


\subsection{Stationary Covariance Function}
Does only depend on $\tau = x - x'$.

\subsubsection{The Matérn Class of Covariance Functions}

A expression for the Matérn covariance function is given by \citeauthor{rasmussen_gaussian_2006}:

\begin{gather*}
    k_{\nu}(\tau) = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}(\frac{\sqrt{2\nu} \tau}{l})^{\nu} K_{\nu}
    (\frac{\sqrt{2\nu} \tau}{l})
\end{gather*}
where $\nu$ and $l$ are positive parameters, $K_{\nu}$ is a modified Bessel function and
$\sigma^2$ = $k_{\nu}(0)$

For $\nu = r + 1/2, r \in \mathbb{N}$ the expression for the Matérn covariance function can be simplified:

\begin{gather}\label{kernel-matern}
    k_{\nu=r+1/2}(\tau) = \sigma^2 \exp(-\frac{\sqrt{2r + 1} \tau}{l}) \frac{r!}{(2p)!}
    \sum_{i=0}^{r} \frac{(r+i)!}{i!(r-i)!}(\frac{2 \sqrt{2 r + 1} \tau}{l})^{r-i}
\end{gather}


Or also with \texttt{includegraphics}:
\begin{figure}[hbt!]%--- Picture 'H'ere, 'B'ottom or 'T'op; '!' Try to
                    %impose your will to LaTeX
  \centering
  \includegraphics[width=\textwidth]{Matern_3} %<< no file extension
  %%         --- .5\textwidth stands for 50% of text width
  \caption[Geyser data: binned histogram, Silverman's and another
  kernel]%<<-- Legend for the list of figures at the beginning of you thesis
  {Matérn kernel function and sample path for different $\nu$}% legend displayed below the graph.
  \label{fig:matern}
\end{figure}


Setting $\nu = 1/2$ with input domain $X \subset \mathbb{R}$ gives raise to a continuous-time AR(1) process,
also called Ornstein-Uhlenbeck process.
With $\nu = 1/2$, i.e. $r=0$, the Matérn covariance function is given by:
\begin{gather}\label{kernel-matern-ar1}
    k(\tau) = \sigma^2 exp(- \tau/l)
\end{gather}

The autocovariance function of an Ornstein-Uhlenbeck process can be derived by solving the stochastic differential equation (SDE) that defines the process.

Starting with the SDE for an OU process:

$$dX_t = \theta (\mu - X_t)dt + \sigma_w dW_t,$$

where $X_t$ is the value of the process at time $t$, $\theta$ is a positive constant that determines the speed of mean reversion,
$\mu$ is the long-term mean of the process, $\sigma_w$ is the standard deviation of the random shocks, and $W_t$ is a standard Wiener process or Brownian motion.

Let $Y_t = X_t - \mu$, then we have:

$$dY_t = dX_t$$
$$= \theta (\mu - X_t)dt + \sigma_w dW_t$$
$$= \theta (-Y_t)dt + \sigma_w dW_t$$

This is a linear stochastic differential equation with constant coefficients, which can be solved using the method of integrating factors. We multiply both sides of the equation by $e^{\theta t}$ and integrate from 0 to $t$:

$$\int_{0}^{t} e^{\theta s} dY_s = -\int_{0}^{t} \theta e^{\theta s} Y_s ds + \int_{0}^{t} \sigma_w e^{\theta s} dW_s$$

Using the fact that $\int_{0}^{t} \sigma_w e^{\theta s} dW_s$ is a Gaussian random variable with mean 0 and variance $\frac{\sigma_w^2}{2\theta}(1 - e^{-2\theta t})$, we can solve for $Y_t$ and get:

$$Y_t = e^{-\theta t} Y_0 + \frac{\sigma_w}{\sqrt{2\theta}}\int_{0}^{t} e^{-\theta(t-s)} dW_s$$

where $Y_0 = X_0 - \mu$.

Now, we can use the definition of covariance to find the autocovariance function of the OU process:

$$Cov(X_t, X_{t-k}) = E[(X_t - \mu)(X_{t-k} - \mu)]$$

$$= E[(Y_t + \mu)(Y_{t-k} + \mu)]$$

$$= E[Y_t Y_{t-k}] + \mu^2$$

Substituting the expression for $Y_t$ and $Y_{t-k}$ from the above equation, we get:

$$Cov(X_t, X_{t-k}) = e^{-\theta k} Var(Y_0) = \frac{\sigma_w^2}{2\theta} e^{-\theta k}$$

where we have used the fact that $Var(Y_0) = \frac{\sigma_w^2}{2\theta}$.

Therefore, the autocovariance function of an OU process is given by
$Cov(X_t, X_{t-k}) = \frac{\sigma_w^2}{2\theta} e^{-\theta k}$,
where $k\geq 0$ and $\theta > 0$. The process is covariance stationary if $\theta > 0$.

This is the same expression as we have obtained in \ref{kernel-matern-ar1}, where
$k(0) = \sigma^2 = \frac{\sigma_w^2}{2\theta}$ and $l=1/\theta$


To see how the Ornstein-Uhlenbeck can be considered a continuous time analogue to the discrete time
AR(1) process one can use the Euler-Maryuama discretization of the process.
Considering again the SDE for an OU process:
$$dX_t = \theta (\mu - X_t)dt + \sigma_w dW_t,$$
The process can be discretized at times $(k \Delta t)_{k \in \mathbb{N}_0}$:

$$ X_{k+1} - X_k = \theta \mu \delta t - \theta X_k \Delta t + \sigma_w (W_{k+1} - W_k)$$

The random variables $(W_{k+1} - W_k)$ are independent and identically distributed normal random variables
with expected value zero and variance $\Delta t$.
Therefore, we can set $\sigma_w (W_{k+1} - W_k) = \sigma_w \sqrt{\Delta t} \epsilon$ with $\epsilon \sim \N(0,1)$
to obtain the following recursion:
$$ X_{k+1} = \theta \mu \Delta t - (\theta \Delta t - 1) X_k + \sigma_w \sqrt{\Delta t} \epsilon$$

The recursion for an AR(1) process is:
$$ X_{k+1} = c + a X_k + b \epsilon$$
Which is identical to the expression above if $c= \theta \mu \Delta t$, $a=- \theta \Delta t$ and
$b= \sigma_w \sqrt{\Delta t}$










More generally a continuous AR(p) process has Matérn covariance function with $\nu = p - 1/2$.

TODO: different kernels, stationary kernels, link to power spectral density, with plots

A more detailed description of covariance functions and their property can be found
in \citeauthor{rasmussen_gaussian_2006} (chapter 4).

\section{Model Selection - Choosing Hyperparameters}
\citeauthor{rasmussen_gaussian_2006} p.113.

Hyperparameters $\theta$

Use marginal likelihood (or evidence):
\begin{gather}
    log p(\mathbf{Y} | X, \theta) = - \frac{1}{2} \mathbf{Y}^{\top} K_{XX}^{-1}(\theta) \mathbf{Y} -
    \frac{1}{2} log \det{K_XX(\theta)} - \frac{n}{2} \log 2 \pi
\end{gather}



%This enforces the kernel function to be symmetric and positive definite.


%which enforces
%certain properties on these kernel functions. A valid kernel function must be:
%
%\begin{itemize}
%    \item symmetric
%    \item postive definite
%\end{itemize}
%

Additionally one can combine valid covariance functions, $k1$ and $k2$ into new covariance functions based on the following rules:
\begin{itemize}
    \item
\end{itemize}



TODO
One could hence also assume additional measurement noise:
\begin{align*}
    Z(x) = Y(x) + \epsilon = f(x) + E(x) + \epsilon  && \epsilon \sim \N(0, \sigma_n^{2})
\end{align*}




