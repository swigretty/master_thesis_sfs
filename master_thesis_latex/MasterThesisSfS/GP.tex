\chapter{Gaussian Process Regression}\label{ch:gaussian-process-regression}


We again consider a regression problem of mapping the input $x$ to an output $f(x)$.
In order to solve such a problem one usually needs some additional constraints on the $f(x)$.
In \ref{ch:time-series-decomposition-and-regression} we restricted ourselves to the class of linear functions.
Another approach is to assign a prior probability to every possible function, where higher
probabilities are assigned to functions that are assumed to be more likely.
Inference in this Bayesian setting is then based on the posterior distribution of these
functions given some potentially noisy observations of $f(x)$.

This chapter first provides a definition of a Gaussian Process and then describes how they
can be used to solve a regression problem.

\section{Gaussian Process Definition}\label{sec:gaussian-process-definition}

A Gaussian process (GP) can be viewed as a gaussian distribution over functions or as an infinite set of random
variables representing the values of the function $f(x)$ at location $x$.
The Gaussian process is thus a generalization of the Gaussian distribution and a formal definition is given
by \citeauthor{rasmussen_gaussian_2006} :

\begin{definition}[Gaussian Process]\label{def:GP}
 A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.
\end{definition}


As a (multivariate) Gaussian distribution is defined by its mean and covariance matrix, a GP is
uniquely identified by its mean $m(x)$ and covariance (kernel) function $k(x,x')$.

We write

\begin{gather*}
    f(x) \sim GP(m(x), k(x,x'))
\end{gather*}
with
\begin{gather*}
    m(x) = \mathbb{E}[f(x)] \\
    k(x,x') = \mathbb{E}[(f(x)-m(x))(f(x')-m(x'))]
\end{gather*}

If we assume $X$ to be the index set or set of possible inputs of $f$, then there is a random variable
$F_x := f(x)$ such that for a set $A \subset X$ with $A={x_1, \dots x_n}$ it holds that:

\[F_A = [F_{x_1}, \dots , F_{x_n}] \sim \N(\mu_A,\,K_{AA})\]
for
\begin{gather}
    K_{AA} =
    \begin{bmatrix}
        k(x_1, x_1) & k(x_1, x_2) & \dots & k(x_1, x_n)\\
        \vdots  &  & \vdots  & \vdots \\
        k(x_n, x_1)  & k(x_n, x_1) & \dots  & k(x_n, x_n)
    \end{bmatrix} \text{and }
    \mu_A =
    \begin{bmatrix}
        \mu(x_1) \\
        \vdots \\
        \mu(x_n)
    \end{bmatrix}
\end{gather}
The finite marginals $F_{x_1}, \dots, F_{x_n}$ of the GP thus have a multivariate gaussian distribution.
In our running example we consider $X$ to be the time interval $T_0=[0, T]$ however it could be higher dimensional.

Note that Gaussian processes with finite index sets and hence with joint gaussian distribution is just a specific case
of GP. If we assume an ARMA process with gaussian innovations for the blood pressure time series, one can view the time series
as collection of normally distributed random variables and are thus dealing with a GP.


If we consider the linear regression case from chapter \ref{ch:time-series-decomposition-and-regression} and assume a
prior distribution
on the $\beta$'s, i.e. $\beta \sim N(0, I)$ then the predictive distribution over $\mu = X \beta$ is Gaussian:
\[
    \mu \sim \N(0, XX^{\top})
\]
and represents a GP with mean function $m(x) = 0$ and kernel function $k(x, x') = x^{\top}x'$.
This special case of gaussian process regression with this specific kernel function is known as baysian linear regression
and will be presented in the next section.


\section{Gaussian Process Regression}
Predictions in the Bayesian regression setting is finding the posterior distribution of
$f^{\ast} := f(x^{\ast})$ at some input $x^{\ast}$, given some potentially noisy observations of $f(x)$.
This is made possible by employing a prior distribution
over the function $f(x)$. As shown in section \ref{sec:gaussian-process-definition}, a GP is essentially
assuming a Gaussian distribution over functions.


\subsection{Bayesian Linear Regression}

We consider again the linear regression model from chapter \ref{ch:time-series-decomposition-and-regression}.
However, we assume a more general setting, where the data generating process does not need to be a time series process.
We denote the mean function with $f(x)$ instead of $\mu(t)$ and $Y_i$ is again a noisy observations of
$f(x_i)$, where the additive error $E_i$ does not necessarily need to arrive from a time series process $E(t)$.
We obtain the following data generating model:
\begin{align*}
    f(x_i) &= x_i^{\top}\beta, & Y_i &= f(x_i) + E_i,  & (i = 1, \dots n)
\end{align*}
with $x_i \in \mathbb{R}^p$ being again the input vector and $\beta \in \mathbb{R}^p$ is the vector with
the regression coefficients.

In matrix from:
\begin{align*}
    \mathbf{Y} = X \beta + \mathbf{E}
\end{align*}
Where $\mathbf{Y} = [Y_{1}, \dots Y_{n}]^{\top}$ is the observed data,
$X = [x_{1}, \dots x_{n}]^{\top} \in \mathbb{R}^{n \times p}$ is the design matrix.
We asume again gaussian but potentially correlated errors $\mathbf{E} = [E_{1}, \dots E_{n}]^{\top}$:
\begin{gather*}
    \mathbf{E} \sim \N(0, \Sigma_n)
\end{gather*}
If $\mathbf{E}$ is an ARMA process with gaussian innovations,
$\mathbf{E}$ being a sum of innovations, will also have gaussian distribution and
this assumption is thus valid.

The likelihood of the observations $\mathbf{Y}$ given $X$ and $\beta$ is then:

\begin{gather*}
    p(\mathbf{Y}|X,\beta) =
    = \frac{1}{( (2\pi)^{n/2} \sqrt {\det(\Sigma_n))}}
    \exp(-\frac{1}{2}(y - X\beta)^{\top} \Sigma_n^{-1}(y-X\beta))
    = \N(X \beta, \Sigma_n)
\end{gather*}

The Bayesian approach differs from the on in chapter \ref{ch:time-series-decomposition-and-regression} in
that we additionally assume a prior distribution over the regression coefficients $\boldsymbol{\beta}$, based on
what we believe are likely values for the coefficients.
To stay in the realm of gaussian processes the prior has to be guassian and we choose:

\begin{gather*}
    p(\boldsymbol{\beta}) = \N(0, \Sigma_p)
\end{gather*}

Given our observations $\mathbf{Y}$  we can use Bayes' theorem to calculate the posterior distribution over $\beta$:
\begin{gather*}
    p(\boldsymbol{\beta}| \mathbf{Y}, X) = \frac{p(\mathbf{Y},\boldsymbol{\beta}|X)}{p(\mathbf{Y}|X)} =
    \frac{p(\mathbf{Y}|X,\boldsymbol{\beta})p(\boldsymbol{\beta})}{p(\mathbf{Y}|X)}
\end{gather*}

One approach is to just plug in the expressions for
$p(\mathbf{Y}|X,\boldsymbol{\beta})$ and $p(\boldsymbol{\beta}|\mathbf{Y}, X)$ from above, with:

\begin{gather*}
    p(\mathbf{Y}|X) = \int p(\mathbf{Y}|X,w) p(w) dw = \N(0, X \Sigma_p X^{\top} + \Sigma_n)
\end{gather*}

Or it can be helpful to combine the coefficients and the observations into a single random vector with
multivariate normal distribution:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        \boldsymbol{\beta}
    \end{bmatrix}
    = \begin{bmatrix} X \\ I_p \end{bmatrix} \boldsymbol{\beta} + \begin{bmatrix} I_n \\ 0 \end{bmatrix}  \mathbf{E}
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        \vdots \\
        \vdots \\
        0 \\
        0 \\
        \vdots \\
        0
        \end{bmatrix},
        \left[
        \begin{array}{ c:c c c }
            \begin{matrix}
                & & \\
                & & \\
                & & \\
                & X \Sigma_p X^{\top} + \Sigma_n & \\
                & & \\
                & & \\
                & & \\
            \end{matrix}
            & \begin{matrix} \\ \\ \\ X \Sigma_p  \\ \\ \\ \end{matrix} \\
            \hdashline \\
            \begin{matrix} &  \Sigma_p X^{\top} & \end{matrix} & \Sigma_p
        \end{array}
        \right]
        \right)
    = p(\mathbf{Y}, \boldsymbol{\beta} | X)
\end{gather}

with $\Sigma_p X^{\top} + \Sigma_n \in \mathbb{R}^{n\times n}$ and $\Sigma_p X^{\top} \in \mathbb{R}^{p\times n}$.

To find now the posterior distribution $p(\boldsymbol{\beta} |  \mathbf{Y}, X)$ one can use the well known rules for deriving conditional
distributions for jointly Gaussian random vectors, which will be presented quickly.
Let's say that $\mathbf{A} \sim \N(\mu_A, \Sigma_{AA})$ and $\mathbf{B} \sim \N(\mu_B, \Sigma_{BB})$ are
Gaussian random vectors with the following joint distribution:

\begin{gather}
    p(\mathbf{A}, \mathbf{B}) = \N(
    \begin{bmatrix}
        \mu_A \\
        \mu_B
    \end{bmatrix},
    \begin{bmatrix}
        \Sigma_{AA} & \Sigma_{AB} \\
        \Sigma_{BA} & \Sigma_{BB}
    \end{bmatrix}
\end{gather}
The distribution $p(\mathbf{B} | \mathbf{A}=a)$ is also normally distributed with mean $\bar{\mu}$ and covariance
$\bar{\Sigma}$ of the following form:

\begin{align}\label{def:conditioning}
    \bar{\Sigma} = \Sigma_{B B} - \Sigma_{B A} \Sigma_{A A}^{-1} \Sigma_{A B} & & \bar{\mu} = \mu_{B} + \Sigma_{BA} \Sigma_{AA}^{-1}(a - \mu_A)
\end{align}


Using this rule we obtain for the mean and covariance of $p(\boldsymbol{\beta} | \mathbf{Y}=y, X) \sim \N(\bar{\mu}, \bar{\Sigma})$:

\begin{align*}
    \bar{\Sigma} = \Sigma_{p} - \Sigma_p X^{\top}(X \Sigma_p X^{\top} + \Sigma_n)^{-1} X  \Sigma_p & & \bar{\mu} = \mu_{\beta} + \Sigma_p X^{\top}(X \Sigma_p X^{\top} + \Sigma_n)^{-1}y
\end{align*}

and using Woodbury matrix identity we can further simplify:
\begin{align*}
    \bar{\Sigma} = (X^{\top}\Sigma_n^{-1}X + \Sigma_p^{-1})^{-1} & & \bar{\mu} = \bar{\Sigma} X^{\top} \Sigma_n^{-1} y
\end{align*}

Deriving the predictive distribution of $f^{\ast} := f(x^{\ast})$ at $x^{\ast}$ given our
observations is then straight forward:

\begin{gather}\label{def:predictive-dist}
    p(f^{\ast} | \mathbf{Y}, X, x^{\ast}) = \N(x^{\ast^{\top}} \bar{\mu}, x^{\ast^{\top}} \bar{\Sigma} x^{\ast})
\end{gather}

One can also use the rules for conditioning to directly derive $f^{\ast} | \mathbf{Y}, X, x^{\ast}$.
Similarly to before we can write the joint distribution $p(\mathbf{Y}, f^{\ast}| X, x^{\ast})$:


\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        f^{\ast}
    \end{bmatrix}
    = \begin{bmatrix} X \\ x^{\ast} \end{bmatrix} \boldsymbol{\beta} + \begin{bmatrix} I_n \\ 0 \end{bmatrix}  \mathbf{E}
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        \vdots \\
        \vdots \\
        0 \\
        0
        \end{bmatrix},
        \left[
        \begin{array}{ c:c c c }
            \begin{matrix}
                & & \\
                & & \\
                & & \\
                & X \Sigma_p X^{\top} + \Sigma_n & \\
                & & \\
                & & \\
                & & \\
            \end{matrix}
            & \begin{matrix} \\ \\ \\ X \Sigma_p x^{\ast} \\ \\ \\ \end{matrix} \\
            \hdashline \\
            \begin{matrix} &  x^{\ast^{\top}}  \Sigma_p X^{\top} & \end{matrix} & \Sigma_p
        \end{array}
        \right]
        \right)
    = p(\mathbf{Y}, f^{\ast}| X, x^{\ast})
\end{gather}

The expression in \ref{def:predictive-dist} can then be derived similarly to before using the rules for
conditioning of gaussian random vectors described in \ref{def:conditioning}.

\subsection{Gaussian Process Regression}
By abandoning the parameters $\beta$, in Gaussian process regression we are always
directly aiming for the predictive distribution of
$f^{\ast} := f(x^{\ast})$ at $x^{\ast}$ given our observations.

Let us first transform the Bayesian linear regression example from the last section
into a GP regression problem.

We had:
\begin{gather*}
    \mathbf{Y}|X \sim \N(0,  X \Sigma_p X^{\top} + \Sigma_n) \\
    \text{and} \\
    \mathbf{f}|X \sim \N(0,  X \Sigma_p X^{\top})
\end{gather*}

Alternatively we can assume a distribution over the function $\mathbf{f(x)}$:

\begin{gather*}
    f(x) \sim GP(0, k(x, x'))
\end{gather*}
where $k(x,x')$ needs to be chosen such that for an input X we obtain $K_{XX} =  X \Sigma_p X^{\top}$

%Assuming $\Sigma_p = \sigma_p I$ and $\Sigma_n = \sigma_n I)$ we get for the kernel function:
%
%\begin{gather*}
%    k(x, x') = \sigma_p x^{\top} x' +  \mathbbm{1}_{x = x'}\sigma_n
%\end{gather*}

%Î´ pq is a Kronecker delta which is one iff p = q and zero otherwise.

If we combine $f^{\ast}$ and $\mathbf{Y}$ into a single random vector:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        f^{\ast}
    \end{bmatrix} =
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        0
        \end{bmatrix},
        \begin{bmatrix}
        K_{XX} + \Sigma_n & K_{Xx^{\ast}} \\
        K_{x^{\ast}X} & K_{x^{\ast}x^{\ast}}
        \end{bmatrix}
        \right)
    = p(\mathbf{Y}, f^{\ast}| X, x^{\ast})
\end{gather}

The conditioning rules can then be used to derive the posterior predictive distribution
$p(f^{\ast} | \mathbf{Y}, X, x^{\ast})$:

\begin{align}
    p(f^{\ast} | \mathbf{Y}, X, x^{\ast}) = \N(x^{\ast^{\top}} \bar{\mu}, x^{\ast^{\top}} \bar{\Sigma} x^{\ast}) \\
    \text{with}: \\
    \bar{\Sigma} = (X^{\top}\Sigma_n^{-1}X + \Sigma_p^{-1})^{-1} & & \bar{\mu} = \bar{\Sigma} X^{\top} \Sigma_n^{-1} y
\end{align}

In time series modelling we might say that $\E$ is not simply an error term but is actuall part of our signal which
we want to predict. Hence we are not only interested in $p(f^{\ast} | \mathbf{Y}, X, x^{\ast})$ but also
in the predictive distribution of
$\mathbf{Y^{\ast}} | \mathbf{Y}, X, x^{\ast})$ with $\mathbf{Y^{\ast}} = \mathbf{Y}(x^{\ast}) = f(x^{\ast} + \mathbf{E}(x^{\ast}))$:

We had:
\begin{gather*}
    \mathbf{Y}|X \sim \N(0,  X \Sigma_p X^{\top} + \Sigma_n) \\
\end{gather*}

Alternatively we can assume a distribution over the function $\mathbf{Y}(x)$:

\begin{gather*}
    \mathbf{Y}(x) \sim GP(0, k(x, x'))
\end{gather*}
where $k(x,x')$ needs to be chosen such that for an input X we obtain $K_{XX} =  X \Sigma_p X^{\top} + \Sigma_n$

If we combine $\mathbf{Y}^{\ast}$ and $\mathbf{Y}$ into a single random vector:

\begin{gather}
    \begin{bmatrix}
        \mathbf{Y} \\
        \mathbf{Y}^{\ast}
    \end{bmatrix} =
    \sim \N \left(
        \begin{bmatrix}
        0 \\
        0
        \end{bmatrix},
        \begin{bmatrix}
        K_{XX} & K_{Xx^{\ast}} \\
        K_{x^{\ast}X} & K_{x^{\ast}x^{\ast}}
        \end{bmatrix}
        \right)
    = p(\mathbf{Y}, f^{\ast}| X, x^{\ast})
\end{gather}


One could hence also assume additional measurement noise:

\begin{align*}
    \mathbf{Z}(x) = \mathbf{Y}(x) + \epsilon = f(x) + \mathbf{E}(x) + \epsilon  && \epsilon \sim \N(0, \sigma_n^{2})
\end{align*}





