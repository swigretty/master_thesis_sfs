\usepackage{amsmath}\chapter{Methods}\label{ch:methods}

The last chapter introduced Gaussian process regression
to establish a mapping between a time point x and its
corresponding BP value.
Since Gaussian Processes are capable of modeling time series in continuous
time and hence deal with irregularly spaced data,
they seem to be a good candidate for modeling a time series from which
we only have irregularly sampled noisy measurement.


To identify how suited GPs are for the problem at hand, the plan is to:
\begin{itemize}
    \item Simulate a BP time series and some noisy measurements of the time series
    (section \ref{sec:blood-pressure-time-series-simulation})
    \item Estimate some target measures from these measurements.
    (section \ref{sec:evaluation-gaussian-process-regression})
    \item Compare the aptness of GP regression to estimate the target measures
    to that of some baseline methods (section \ref{sec:baseline-methods}).

\end{itemize}

Additionally, the impact of different adversarial factor on the target measure estimate
should be investigated.
This will be described in section \ref{sec:adversarial-analysis}.


\section{Problem Statement}

Recall the problem statement from section \ref{sec:problem-statement}.
Firs, we assumed the following model for the BP measurements
$Y(x)$ at a time point $x$:

\begin{align*}
    Y(x) = f(x) + \epsilon && \epsilon \sim \N(0, \sigma_n^{2})
\end{align*}

where $f(x)$ denotes the true BP process and $\epsilon$ is iid measurement noise,
independent from $f(x)$.

The goal is to estimate the true BP values $f(x)$ at some input time $X$,
based on noisy observations of $f(x)$ at some training time points $X_{train}$.
For the sections of this chapter we define:
\begin{description}
    \item $X := \{ x_1 \dots x_n \}$: An index set, that spans
    the one-week time range of interest, with 10 BP values per hour.\\

    \item $X_{train} \subset X$: The training indexes. \\

    \item $G_X := (g(x) : x \in X )$ for some function $g(x)$.
    Specifically:
    \begin{itemize}
        \item $F_X := (f(x) : x \in X )$: The true BP values at inputs $X$.
        \item $Y_X= (Y(x) : x \in X )$: The noisy BP measurements at inputs $X$.
        \item $Y_{X_{train}} := (Y(x) : x \in X_{train})$: The data used for estimating $F_X$
    \end{itemize}
    
    \item $\text{ }$\\

    \item $K_{XX'} := \begin{bmatrix}
            k(x_1, x'_1) & \dots & k(x_1, x'_n)\\
            \vdots  &  & \vdots \\
            k(x_n, x'_1) & \dots  & k(x_n, x'_n)
         \end{bmatrix}$ ,for some kernel function, $k(x, x')$ \\

        and some inputs $X=(x_1, \dots x_n)$ and $X'=(x'_1, \dots x'_n)$.
    \item $\text{ }$\\
\end{description}

Additionally, when referring to the estimated values,
$\hat{F}_X$ is used instead of $F_X$.


\subsection{Performance Assessment}
From the estimated BP values, $\hat{F}_X$, some target measures
should be estimated, including 95\%-credible intervals.

By repeated simulations of true BP values, $F_X$, with estimation of the
target measures and their credible intervals, the performance is
assessed using:
\begin{itemize}
    \item CiCoverage: The number of times the true target
    measure value was covered by the credible interval, divided by
    $S$, the number of simulations.
    \item CiWidth: The mean width of the credible interval over $S$ simulations
\end{itemize}


\section{Blood Pressure Time Series Simulation}\label{sec:blood-pressure-time-series-simulation}
For the simulation of the blood pressure time series, the goal is to match the properties
desribed in section \ref{sec:problem-statement}.
Simulation starts by generating the true BP time series process, $f(x)$,
from which $Y(x)$ can be obtained by adding iid Gaussian measurement noise.

The true BP process $f(x)$ was decided to be modeled by a Gaussian process
(true GP),
since GPs are flexible enough to represent the properties
specified for $f(x)$ in section \ref{sec:characteristics-of-the-blood-pressure-time-series}.

\subsection{Mean function}
A reasonable assumption for the mean function, would be to keep it constant and
equal to the global mean BP value of 120 mmHg. We have:
\begin{gather*}
    f(x) \sim GP(120, k(x,x'))
\end{gather*}
From section \ref{subsec:mean-function} we know that this is the same as writing:
\begin{gather*}
    f(x) - 120 \sim GP(0, k(x,x'))
\end{gather*}
For simplicity we are going to completely ignore this constant
mean function throughout the rest of the thesis and
model the true BP process $f(x)$ with the following
GP:
\begin{gather*}
    f(x) \sim GP(0, k(x,x'))
\end{gather*}
where we write $f(x)$, although we actually mean $f(x) - 120$.


\subsection{Kernel function}
The kernel function chosen to match the properties from
section \ref{sec:characteristics-of-the-blood-pressure-time-series} is:
\begin{gather*}\label{def:true_gp}
k(x, x') = 2.24^{2} * \text{Matérn}(l=3, \nu=0.5) +
14^{2} * \text{Periodic}(l=3, p=24) +  2.24^{2} * \text{RBF}(l=50)
\end{gather*}
where l denotes the length scale and p denotes the periodicity
of the corresponding kernel function in hours.
The formal definition of the Matérn, Periodic and RBF kernel
functions and their parameters is provided in section \ref{sec:kernel}.

Each of these kernels models one of the components described in
\ref{sec:characteristics-of-the-blood-pressure-time-series}:
\begin{itemize}
    \item The Matérn kernel with $\nu$=0.5 models the AR(1) component
    \item The Periodic kernel models the circadian cycle
    \item The RBF kernel model a long term trend
\end{itemize}

The kernel function is illustrated in figure \ref{fig:true_kernel} and
some samples drawn form this GP are shown in Figure \ref{fig:true_gp_samples}.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.6\linewidth]{Pictures/plots_final/sin_rbf_default_0.2/09_06_09_09_17/plot_kernel_true}
    \caption{The true kernel function $k(x,x')$}
    \label{fig:true_kernel}
\end{figure}

\begin{figure}[!h]
\centering
\begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Pictures/plots_final/sin_rbf_default_0.05/09_06_08_59_46/plot_true_mean_decomposed}
    \includegraphics[width=\linewidth]{Pictures/plots_final/sin_rbf_default_0.05/09_06_09_00_31/plot_true_mean_decomposed}
    \includegraphics[width=\linewidth]{Pictures/plots_final/sin_rbf_default_0.05/09_06_09_00_58/plot_true_mean_decomposed}
  \caption{The sample $F_X$ shown to the right, decomposed in to the contribution of the Periodic kernel (orange),
      Matérn kernel (blue), RBF kernel (green).}
  \label{fig:true_mean_decomposed}
\end{subfigure}\hfill
\begin{subfigure}{.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{Pictures/plots_final/sin_rbf_default_0.05/09_06_08_59_46/plot_true_with_samples}
    \includegraphics[width=\linewidth]{Pictures/plots_final/sin_rbf_default_0.05/09_06_09_00_31/plot_true_with_samples}
    \includegraphics[width=\linewidth]{Pictures/plots_final/sin_rbf_default_0.05/09_06_09_00_58/plot_true_with_samples}
  \caption{Each figure shows one sample $F_X$ drawn from the true GP (red dashed line) with noisy observations
      (red dots) sampled at a frequency of 0.5/hour}
  \label{fig:sub2}
\end{subfigure}
\caption{Three samples (right side) drawn from the true GP and the decomposition of theses samples (left side)}
\label{fig:true_gp_samples}
\end{figure}



\subsection{Simulation of the BP Measurments}

The BP measurements time series process $Y(x)$ is obtained by adding iid measurement noise
$\epsilon \sim \N(0, \sigma_n^2)$ to $f(x)$.
Where the measurement noise variance $\sigma_n^2$ is set to 31 mmHg².
The rationale for this is given in subsection
\ref{sec:characteristics-of-the-blood-pressure-time-series}.
The measurement indexes $X_{train}$ are then drawn uniformly at random.
from $X$, yielding the training data, $Y_{X_{train}}$.

The distribution of some properties from simulated BP samples can be found in the
Appendix \ref{sec:properties-of-the-simulated-time-series-samples}.


\section{Evaluation of Gaussian Process Regression}\label{sec:evaluation-gaussian-process-regression}

A Gaussian process regression was fitted to $Y_{X_{train}}$ to estimate $F_X$.
The kernel function used, has the same form as the one used for simulation
but with variable hyperparameters:


\begin{gather*}\label{def:gp_fit}
    k(x, x')= \sigma_M^2 * \text{Matérn}(l, \nu=0.5) +
\sigma_P^2 * \text{Periodic}(l, p=24) +  \sigma_R^2 * \text{RBF}(l)
\end{gather*}

The hyperparameters, $\sigma_M^2, \sigma_P^2, \sigma_R^2, l$  were found by
maximizing the marginal likelihood, as described in subsection
\ref{subsec:marginal-likelihood}, yielding the optimal kernel $\hat{k}(x,x')$.

From $\hat{k}(x,x')$ the predictive distribution over $F_X$ was calculated.
Further the target measures including credible intervals were estimated by
sampling form the predictive distribution.
The Pseudocode \ref{pc:simulation-evaluation-flow} describes the whole simulation
flow for evaluating the performance of GP regression for a
specific target measure.

\begin{algorithm}[h!] \caption{Simulation and Evaluation Flow}
 \hspace*{\algorithmicindent} \textbf{Inputs:} \\
 \hspace*{\algorithmicindent} $X$ \algorithmiccomment{The evaluation time points} \\
 \hspace*{\algorithmicindent} $K_{XX}$ \algorithmiccomment{ The true kernel function evaluated at the input $X$}\\
 \hspace*{\algorithmicindent} TargetMeasure \algorithmiccomment{Function to extract target measure from $F_X$ or $\hat{F}_X$} \\
 \hspace*{\algorithmicindent} \textbf{Output:} \\
 \hspace*{\algorithmicindent} CiCoverage \algorithmiccomment{Credible interval coverage}\\
 \hspace*{\algorithmicindent} CiWidth \algorithmiccomment{Credible interval width}\\
\begin{algorithmic}[1]
    \State \textbf{Initialize:} CiCoverageList = $\left[ \text{ } \right]$, CiWidthList = $\left[ \text{ } \right]$,
    \For {$s = 0$ \dots $S$}
        \State $F_X =$ sample from $\N(0, K_{XX})$ \algorithmiccomment{Sample from the true GP}
        \State $X_{train} \subset X$ \algorithmiccomment{Choose training indexes}
        \State $Y_{X_{train}} = F_{X_{train}} + \epsilon$, $\epsilon \sim  \N(0, \sigma_n^2)$
        \State $\hat{k}=$ GP.fit($X_{train}, Y_{X_{train}}$) \algorithmiccomment{Fit the Gaussain process}
        \State $\hat{F}_X = \hat{K}_{XX_{train}} (\hat{K}_{X_{train}X_{train}} + \sigma_n^2 I)^{-1} Y_{train}$ \algorithmiccomment{predictive mean}
        \State $\hat{\Sigma}_{F_X} = \hat{K}_{XX} -\hat{K}_{XX_{train}}(\hat{K}_{X_{train}X_{train}} + \sigma_n^2 I)^{-1}\hat{K}_{X_{train}X}$ \algorithmiccomment{predictive covariance}
        \State \textbf{Initialize:} $\hat{M} = \left[ \text{ } \right]$
        \For {$k = 0$ \dots $K$}
            \State $\hat{F}_{X,k} = $ sample from $\N(\hat{F}_X, \hat{\Sigma}_{F_X} )$ \algorithmiccomment{Sample from predictive distribution}
            \State $\hat{M}$.append(TargetMeasure($\hat{F}_{X, k}$)) \algorithmiccomment{Extract target measure}
        \EndFor
    \State $m =$ TargetMeasure($F_X$) \algorithmiccomment{Extract true target measure}
    \State $\hat{m} = $mean($\hat{M}$)
    \State $ci\_lb = (2\hat{m} - \text{quantile}_{1-\alpha/2}(\hat{M})$ \algorithmiccomment{Credible interval lower bound}
    \State $ci\_ub = (2\hat{m} - \text{quantile}_{\alpha/2}(\hat{M}))$ \algorithmiccomment{Credible interval upper bound}
    \State CiCoverageList.append($ ci\_lb \leq m \leq ci\_ub$)
    \State CiWidthList.append($ci\_ub - ci\_lb $)
    \EndFor
    \State CiCoverage = mean(CiCoverageList)
    \State CiWidth = mean(CiWidthList)
\end{algorithmic}
\end{algorithm}\label{pc:simulation-evaluation-flow}

\subsection{Target Measures}
In subsection \ref{subsec:target-measures} the mean BP over different time
windows and TTR has been defined as the measures of interest. These measures
are extracted from $F_X$ to obtain the true target measure values and
from $\hat{F}_X$ to obtain the estimated target measures.

The \textbf{one-week mean BP}, $\bar{F}_X$, was calculated as the mean of all values in
$F_X$:
\begin{gather*}
    \bar{F}_{X} = \frac{1}{n} \sum_{x \in X} f(x)
\end{gather*}

The \textbf{one-hour and one-day BP means}, $\bar{F}_{X_1} \dots \bar{F}_{X_W}$,
were calculated by taking the mean value of $f(x)$ evaluated at the different
time windows $X_1 \dots X_W \subset X$.
For the first one-hour or one-day widow $X_1$ this is:
\begin{gather*}
    \bar{F}_{X_1} = \frac{1}{n_1} \sum_{x \in X_1} f(x), \\
    \text{with $n_1$ being the number of elements in $X_1$}
\end{gather*}
In the evaluation flow from \ref{pc:simulation-evaluation-flow}, in
each simulation iteration s, a different time window from
$X_1 \dots X_W$ is chosen uniformly at random. Hence, in each simulation
only the mean over one time window is considered and evaluated
but at the end the mean performance over all $S$ simulation is reported in
the form of CiCoverage and CiWidth.


\textbf{TTR} was calculated by deviding the number of BP values in $F_X$,
which are within the target range total number of values in $F_X$.
\begin{gather*}
    \frac{1}{n} \sum_{x \in X} \mathbbm{1}\{\ 90 < f(x) < 125 \}
\end{gather*}

%	Calculate TTR:
%	\begin{itemize}
%			\item General: Number of predicted data points within the range over the total number of data points
%			\item Naive TTR: Number of available data points within the range over the total number of available data points
%			\item True: $\sum_{i=1}^{n} \mathbbm{1}\{\ f(x_i) < \gamma \}$.
%			\item GP/Spline/: $\sum_{i=1}^{n} \mathbbm{1}\{\ \bar{\mu}_i < \gamma \}$. Sample from Posterior to get CI
%			\item Naive: $\sum_{i=1, y_i \in y_{train}}^{m} \mathbbm{1}\{\ y_i < \gamma \}$
%	\end{itemize}



%$F_X$ is replaced by $\hat{F}_X$ for extracting the measures from the
%estimated BP values.

%
%Simulation of true BP signal and measurements:
%    \begin{itemize}
%        \item $X=\{x_1, \dots x_n\}$: The time points of interest, which is one week of data with 10 BP values per hour.
%        \item $f_X := \{f(x_1) \dots f(x_n)\}$: The true BP signal $f(x)$ drawn from $GP(0, k_{true}(x,x'))$ evaluated at inputs $X$.
%        \item $y_X := \{y(x_1) \dots y(x_n)\}$: The noisy BP measurements. $y(x)= f(x) + \epsilon$ with $\epsilon \sim N(0, \sigma_n^2)$
%    \end{itemize}
%
%Based on subsampling scheme choose $X_{train} \subset X$ with $|X_{train}| = m$ and $X_{test} = X \setminus X_{train}$.
%\begin{itemize}
%    \item $y_{train} := \{y(x_i) | x_i \in X_{train}\}$
%    \item  $f_{train} := \{f(x_i) | x_i \in X_{train}\}$
%\end{itemize}
%
%
%	Fit GP regression model to training data $y_{train}$, $X_{train}$:
%    \begin{itemize}
%        \item $k_{fit}$: The fitted kernel function with hyperparameters $\theta$ that maximizes the marginal likelihood
%        $p(y_{train}| \theta)$
%        \item $p(f_X| y_{train}, k_{fit}) = N(\bar{\mu}, \bar{\Sigma})$:
%        The posterior (predictive) probability density of $f_X$. The posterior mean vector
%        $\bar{\mu} \in \mathbb{R}^n$ contains the point estimates for $f_X$.
%        \begin{itemize}
%            \item $\bar{\mu}_{train} := \{\bar{\mu}_i | x_i \in X_{train}\}$
%            \item $\bar{\Sigma}_{train} := \bar{\Sigma}_{i,j}$ where $\{i | x_i \in X_{train}\}$ and $\{j | x_j \in X_{train}\}$
%
%        \end{itemize}
%    \end{itemize}
%
%	Output of GP Regression are predictive probabilities $p(f_{train}| y_{train})$,
%		$p(f_{test}| y_{train})$ and $p(f_{X}| y_{train})$:
%			\begin{itemize}
%				\item Note $\bar{\mu}_{train} \neq f_{train}$ due to the measurement error. \\ If
%				$f_{train}$ was known: $p(f_{train}| f_{train}, GP_{fit}) = N(\bar{\mu}_{X_{train}},
%				\bar{\Sigma}_{X_{train}})$
%				with $\bar{\mu}_{train} = f_{train}$ and $\bar{\Sigma}_{train} = {\displaystyle O}$
%			\end{itemize}
%


\section{Baseline Methods}\label{sec:baseline-methods}

Some other methods were fitted to $Y_{X_{train}}$ as a reference, to which
the GP performance was compared to.

The following baseline methods selected, a linear regression, smoothing spline,
overall mean and naive TTR.

All methods, but naive TTR, estimate the target measure through estimation of $F_X$
The calculation of the target measure and confidence interval is described in
Pseudocode \ref{pc:target-measure-baseline}.
The procedure is equivalent to GP regression, except that one does not sample from
the posterior distribution but uses bootstrap samples instead.


\begin{algorithm}[h!] \caption{Target Measrue Estimation with CI}
 \hspace*{\algorithmicindent} \textbf{Inputs:} \\
 \hspace*{\algorithmicindent} $X$ \algorithmiccomment{The evaluation time points} \\
 \hspace*{\algorithmicindent} $F_X$  \algorithmiccomment{True BP values at inputs $X$} \\
 \hspace*{\algorithmicindent} $X_{train}$ \algorithmiccomment{The training indexes} \\
 \hspace*{\algorithmicindent} RegressionMethod \algorithmiccomment{The baseline method} \\
 \hspace*{\algorithmicindent} TargetMeasure \algorithmiccomment{Function to extract target measure from $F_X$ or $\hat{F}_X$} \\
 \hspace*{\algorithmicindent} \textbf{Output:} \\
 \hspace*{\algorithmicindent} CiCoverage \algorithmiccomment{Credible interval coverage}\\
 \hspace*{\algorithmicindent} CiWidth \algorithmiccomment{Credible interval width}\\
\begin{algorithmic}[1]
    \State $Y_{X_{train}} = F_{X_{train}} + \epsilon$, $\epsilon \sim  \N(0, \sigma_n^2)$
    \State \textbf{Initialize:} $\hat{M} = \left[ \text{ } \right]$
    \For {$k = 0$ \dots $K$}
        \State $X^{\ast} = $ sample with replacement from $X_{train}$
        \State $\hat{F}_{X,k} = $ RegressionMethod.fit($X^{\ast}, Y_{X^{\ast}}$).predict($X$)
        \State $\hat{M}$.append(TargetMeasure($\hat{F}_{X, k}$)) \algorithmiccomment{Extract target measure}
    \EndFor
    \State $m =$ TargetMeasure($F_X$) \algorithmiccomment{Extract true target measure}
    \State $\hat{m} = $mean($\hat{M}$)
    \State $ci\_lb = (2\hat{m} - \text{quantile}_{1-\alpha/2}(\hat{M})$ \algorithmiccomment{Credible interval lower bound}
    \State $ci\_ub = (2\hat{m} - \text{quantile}_{\alpha/2}(\hat{M}))$ \algorithmiccomment{Credible interval upper bound}
    \State CiCoverage = $ ci\_lb \leq m \leq ci\_ub$)
    \State CiWidth = $ci\_ub - ci\_lb $)
\end{algorithmic}
\end{algorithm}\label{pc:target-measure-baseline}




\subsection{Linear Regression}

The model used has already been presented in section \ref{sec:linear-regression} and
it features as linear trend and seasonal component:
\begin{gather*}
Y(x) = \beta_0 + \beta_1 x + \beta_2 \cos(2 \pi f x) + \beta_3 \sin(2 \pi f x) + R(t), \\
\end{gather*}
, where $f$, the frequency, is known and equals $1/\text{period} = 1/24$.









\section{Adversarial Analysis}\label{sec:adversarial-analysis}

Finally we want to study the impact on of the sampling pattern on the target measure estimates.
As described in section \ref{sec:characteristics-of-the-blood-pressure-time-series}
the data density is expected to vary within the Aktiia population and
measurements might
also not be sampled uniformly but data density might follow the circadian cycle.
We will refer to the latter phenomenon as seasonal sampling.
Different degree of data density and seasonal sampling should thus be used to
answer the following questions:
\begin{itemize}
    \item How densely does the data need to be sampled to get good estimates?
    \item What to the target measure estimates in the face of seasonal sampling?
\end{itemize}



\section{Computational Frameworks}

All code has been written in Python.
For Gaussian process simulation and Regression the Pyton package scikit-learn
has been used.
















